{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "6c879503",
   "metadata": {},
   "source": [
    "### RAG Pipelines- Data Ingestion to Vector DB Pipeline"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "8d6377b9",
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "from langchain_community.document_loaders import PyPDFLoader, PyMuPDFLoader\n",
    "from langchain.text_splitter import RecursiveCharacterTextSplitter\n",
    "from pathlib import Path"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "06478b28",
   "metadata": {},
   "source": [
    "### Read all the pdf's inside the directory"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "88a312dc",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Found 1 PDF files to process\n",
      "\n",
      "Processing: A retrieval-augmented generation based large language model benchmarked on a novel dataset.pdf\n",
      "  ‚úì Loaded 9 pages\n",
      "\n",
      "Total documents loaded: 9\n"
     ]
    }
   ],
   "source": [
    "\n",
    "def process_all_pdfs(pdf_directory):\n",
    "    \"\"\"Process all PDF files in a directory\"\"\"\n",
    "    all_documents = []\n",
    "    pdf_dir = Path(pdf_directory)\n",
    "    \n",
    "    # Find all PDF files recursively\n",
    "    pdf_files = list(pdf_dir.glob(\"**/*.pdf\"))\n",
    "    \n",
    "    print(f\"Found {len(pdf_files)} PDF files to process\")\n",
    "    \n",
    "    for pdf_file in pdf_files:\n",
    "        print(f\"\\nProcessing: {pdf_file.name}\")\n",
    "        try:\n",
    "            loader = PyPDFLoader(str(pdf_file))\n",
    "            documents = loader.load()\n",
    "            \n",
    "            # Add source information to metadata\n",
    "            for doc in documents:\n",
    "                doc.metadata['source_file'] = pdf_file.name\n",
    "                doc.metadata['file_type'] = 'pdf'\n",
    "            \n",
    "            all_documents.extend(documents)\n",
    "            print(f\"  ‚úì Loaded {len(documents)} pages\")\n",
    "            \n",
    "        except Exception as e:\n",
    "            print(f\"  ‚úó Error: {e}\")\n",
    "    \n",
    "    print(f\"\\nTotal documents loaded: {len(all_documents)}\")\n",
    "    return all_documents\n",
    "\n",
    "# Process all PDFs in the data directory\n",
    "all_pdf_documents = process_all_pdfs(\"../data\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "96748c96",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[Document(metadata={'producer': 'Adobe PDF Library 23.8.53', 'creator': 'Acrobat PDFMaker 23 for Word', 'creationdate': '2024-02-21T12:54:50-06:00', 'author': 'JOFSR', 'comments': '', 'company': '', 'keywords': '', 'moddate': '2024-02-21T12:55:38-06:00', 'sourcemodified': 'D:20240221185350', 'subject': '', 'title': '', 'source': '..\\\\data\\\\pdf\\\\A retrieval-augmented generation based large language model benchmarked on a novel dataset.pdf', 'total_pages': 9, 'page': 0, 'page_label': '1', 'source_file': 'A retrieval-augmented generation based large language model benchmarked on a novel dataset.pdf', 'file_type': 'pdf'}, page_content='A Retrieval-Augmented Generation Based Large \\nLanguage Model Benchmarked on a Novel Dataset \\nKieran Pichai \\nMenlo School \\nAB\\nSTRACT \\nThe evolution of natural language processing has seen marked advancements, particularly with the advent of models \\nlike BERT, Transformers, and GPT variants, with recent additions like GPT  and Bard. This paper investigates the \\nRetrieval-Augmented Generation (RAG) framework, providing insights into its modular design and the impact of its \\nconstituent modules on performance. Leveraging a unique dataset from Amazon Rainforest natives and biologists, our \\nresearch demonstrates the signiÔ¨Åcance of preserving indigenous cultures and biodiversity. The experiment employs a \\ncustomizable RAG methodology, allowing for the interchangeability of various components, such as the base language \\nmodel and similarity score tools. Findings indicate that while GPT performs slightly better when given context, Palm \\nexhibits superior performance without context. The results also suggest that models tend to perform optimally when \\npaired with similarity scores from their native platforms. Conclusively, our approach show cases the potential of a \\nmodular RAG design in optimizing language models, presenting it as a more advantageous strategy compared to tra-\\nditional Ô¨Åne-tuning of large language models. \\nIntroduction \\nThe evolution of natural language processing models has seen signiÔ¨Åcant strides from rule -based approaches in the \\nearly stages of language understanding, eventually leading to the advent of neural networks. However, the full potential \\nof these neural netw orks awaited the computational infrastructure to catch up. The pivotal moment arrived with the \\nemergence of neural machine translation (NMT), exempliÔ¨Åed by Google Translate, which marked a turning point in \\nmachine language comprehension (Bahdanau, 2016). Subsequently, a plethora of advanced models, including BERT, \\nTransformers, GPT-2, and GPT-3, have emerged, driving the Ô¨Åeld forward. Recent notable additions to this landscape \\nare models like GPT and Bard (Devlin, 2018) (Vaswani, 2017) (Radford, 2018). While Ô¨Åne-tuning such models has \\nproven to be a challenging endeavor, it has become evident that Retrieval-Augmented Generation (RAG) oÔ¨Äers a prom-\\nising alternative (Lewis, 2020) (Siriwardhana, 2023) (Yu, 2022). \\nCuriously, little attention has been devoted to dissecting the individual components of RAG and their respec-\\ntive impacts on overall performance. In response to this gap, our paper undertakes a comprehensive investigation of \\nthe RAG framework and embarks on the design of RAG models from the ground up, with a focus on the modularity \\nand replaceability of its constituent modules. This research seeks to contribute to a deeper understanding of the mech-\\nanisms underlying RAG and its potential for enhancing natural language understanding and generation. These Large \\nLanguage Models (LLMs) exhibit a remarkable proÔ¨Åciency in replicating human language styles, achieving a level of \\nlinguistic verisimilitude that borders on the impeccable. In light of these capabilities, it is prudent to delve into the \\nprofound signiÔ¨Åcance of the Amazon rainforest, which equates to the importance of any ethnically or racially diverse \\nnation across the globe. Within the vast expanse of the Amazon, an intricate tapestry of life unfolds, where millions \\nof distinct species intermingle. Each of these species, as rare as the other, holds a unique and intrinsic value to the \\nindigenous populations who have made this ecosystem their home. The Amazon rainforest is not only a cradle of \\nbiological diversity but also a sanctuary for an array of religions and cultures, many of which teeter on the brink of \\nVolume 12 Issue 4 (2023) \\nISSN: 2167-1907\\nwww.JSR.org/hs\\n1'),\n",
       " Document(metadata={'producer': 'Adobe PDF Library 23.8.53', 'creator': 'Acrobat PDFMaker 23 for Word', 'creationdate': '2024-02-21T12:54:50-06:00', 'author': 'JOFSR', 'comments': '', 'company': '', 'keywords': '', 'moddate': '2024-02-21T12:55:38-06:00', 'sourcemodified': 'D:20240221185350', 'subject': '', 'title': '', 'source': '..\\\\data\\\\pdf\\\\A retrieval-augmented generation based large language model benchmarked on a novel dataset.pdf', 'total_pages': 9, 'page': 1, 'page_label': '2', 'source_file': 'A retrieval-augmented generation based large language model benchmarked on a novel dataset.pdf', 'file_type': 'pdf'}, page_content='oblivion. Preserving the Amazon is not merely an environmental imperative; it is an act of justice to the indigenous \\ncommunities whose ancestral lands are enshrined within its boundaries. It is a call to safeguard the memories of the \\nland, the traditions that have evolved within its embrace, and the very essence of their cultures. However, certain \\nregions of the Amazon remain shrouded in obscurity, their Ô¨Çora and fauna so rare that reliable and readily available \\ninformation is conspicuously lacking in the vast repository of knowledge available on the internet. In this context, \\nadvanced LLMs play an instrumental role in addressing this deÔ¨Åcit by facilitating the dissemination of indigenous \\nnarratives and thereby amplifying awareness and appreciation of the rich tapestry of beliefs, practices, and traditional \\nknowledge that these communities hold dear. They serve as a bridge connecting the indigenous Amazonian cultures \\nwith the global community, emphasizing the paramount importance of preserving the cultural diversity interwoven \\nwithin this vast rainforest. In sum, the overarching mission of this endeavor is twofold: to document and educate the \\nWestern world about hitherto unknown cultures while concurrently ensuring the enduring preservation of these inval-\\nuable facets of human heritage and biodiversity. \\n \\nProposed Experiment \\n \\nBackground and Importance \\n \\nThe intrinsic value of indigenous knowledge, especially from regions as biodiverse and culturally rich as the Amazon \\nRainforest, cannot be overstated. This knowledge, passed down through generations, encompasses not only cultural \\nand religious beliefs but also practical insights into the local Ô¨Çora and fauna. As the modern world encroaches on these \\nlands, this wisdom is in peril of being lost forever. Recognizing this, our proposed experiment aims to employ a state-\\nof-the-art Retrieval- Augmented Generation (RAG) framework to capture and leverage this  vast, yet vulnerable, \\nknowledge base. \\nOur dataset, derived from interviews with Amazon Rainforest natives and biologists, is unparalleled in its \\ndepth and breadth. It includes detailed discussions on religious practices, cultural nuances, and the integral role of the \\nsurrounding ecosystem in the daily lives of these communities. This data is not just a scientiÔ¨Åc or anthropological \\nresource; it is a repository of living history and an urgent call to action for preservation eÔ¨Äorts. \\nBy integrating this unique dataset into the RAG framework, we anticipate not only the preservation of \\nknowledge but also the generation of responses that reÔ¨Çect the rich tapestry of Amazonian life. The experiment is \\ndesigned to evaluate how diÔ¨Äerent components within the RAG setup‚Äîsuch as base language models and similarity \\nscoring algorithms‚Äîcan be optimized to reÔ¨Çect the nuances captured within our dataset. In doing so, we aim to bridge \\nthe gap between advanced language models and the profound human insights found within the Amazon. \\nThe central objective of our experiment is twofold: to analyze the performance implications of modular design \\nwithin the RAG framework and to demonstrate the profound capability of such a system to preserve and communicate \\nthe wealth of indigenous knowledge. We hypothesize that a customizable RAG model will not only facilitate a deeper \\nunderstanding of the data but also allow us to Ô¨Åne -tune the system for optimal performance across diÔ¨Äerent conÔ¨Ågu-\\nrations. To achieve this, we will systematically explore the interchangeability of various RAG components. We will \\nassess diÔ¨Äerent base language models such as GPT and Palm and compare the eÔ¨Écacy of similarity scoring tools from \\ndiverse platforms. The experiment will rigorously test these combinations, identifying which synergies most eÔ¨Äectively \\ncapture the essence of the dataset. \\nThe ultimate goal is to showcase the potential of a modular RAG system in processing culturally signiÔ¨Åcant \\ninformation, paving the way for future applications that can beneÔ¨Åt from such tailored language models. We anticipate \\nthat our Ô¨Åndings will contribute signiÔ¨Åcantly to the Ô¨Åelds of computational linguistics and cultural preservation, demon-\\nstrating a novel approach to the application of large language models. \\n \\n \\n \\nVolume 12 Issue 4 (2023) \\nISSN: 2167-1907\\nwww.JSR.org/hs\\n2'),\n",
       " Document(metadata={'producer': 'Adobe PDF Library 23.8.53', 'creator': 'Acrobat PDFMaker 23 for Word', 'creationdate': '2024-02-21T12:54:50-06:00', 'author': 'JOFSR', 'comments': '', 'company': '', 'keywords': '', 'moddate': '2024-02-21T12:55:38-06:00', 'sourcemodified': 'D:20240221185350', 'subject': '', 'title': '', 'source': '..\\\\data\\\\pdf\\\\A retrieval-augmented generation based large language model benchmarked on a novel dataset.pdf', 'total_pages': 9, 'page': 2, 'page_label': '3', 'source_file': 'A retrieval-augmented generation based large language model benchmarked on a novel dataset.pdf', 'file_type': 'pdf'}, page_content=\"Source and Composition \\n \\nOur proprietary dataset stands as the cornerstone of this experiment. It is a rich compendium of verbal histories, inter-\\nviews, and ecological insights gathered from the indigenous peoples of the Amazon Rainforest, as well as from biolo-\\ngists and ecologists dedicated to studying this unique biome. The dataset is characterized by its diversity, comprising \\nnarratives that elucidate the intricate relationship between the natives and their environment, including the religious \\nand cultural signiÔ¨Åcance of plant and animal life. \\nThe data collection was an extensive process, where linguists and researchers engaged in deep conversations \\nwith the natives, recording their dialects, translating their stories, and documenting their knowledge of the ecological \\nsystem. Similarly, biologists contributed their decades of research on the Ô¨Çora and fauna, providing a scientiÔ¨Åc per-\\nspective to the indigenous narratives. The data thus forms a conÔ¨Çuence of traditional wisdom and modern scientiÔ¨Åc \\nunderstanding, oÔ¨Äering a 360-degree view of the Amazon Rainforest's ecosystem. \\n \\nCultural and Environmental SigniÔ¨Åcance \\n \\nThe urgency of preserving indigenous knowledge is akin to conserving an endangered species. It is a race against time, \\nas globalization and environmental degradation threaten to erase unique cultures and the wisdom they hold. Our dataset \\nserves as a digital ark, a means to preserve and perpetuate the knowledge that has sustained the Amazon's communities \\nfor millennia. \\nThe environmental signiÔ¨Åcance of the Amazon Rainforest cannot be overstated‚Äîit is a keystone of global \\nbiodiversity. By documenting the intricate knowledge, the natives have of their environment, we are also chronicling \\nthe ecological interdependencies that are vital for the rainforest's survival. This dataset, therefore, is not just an aca-\\ndemic or technological asset; it is a critical record for environmental conservationists and policymakers. \\nThrough our experiment, we aim to amplify the voices of the Amazon's indigenous peoples, whose under-\\nstanding of their habitat is unmatched. By integrating their knowledge into the RAG framework, we hope to create a \\nmodel that not only responds with information but also with wisdom  that respects the interconnectedness of life and \\nculture. \\n \\nRetrieval-Augmented Generation Framework \\n \\nThe heart of our experiment lies in the Retrieval-Augmented Generation (RAG) framework, a sophisticated algorithm \\nthat enables the deconstruction of the language model into discrete, interchangeable components. This framework \\nintegrates a retriever model t hat sources relevant context and a generator model that synthesizes the retrieved infor-\\nmation into coherent responses. \\nIn mathematical terms, given an input query ùëûùëû, the retriever model searches a knowledge base ùí¶ùí¶ and retrieves \\na set of relevant documents ùê∑ùê∑ = {ùëëùëë1, ùëëùëë2, ‚Ä¶ , ùëëùëëùëòùëò}. Each document ùëëùëë is represented as a vector ùêØùêØùëëùëë in a high-dimensional \\nspace, obtained from an embedding layer. This process transforms the raw text data into a structured form amenable \\nto computational manipulation. \\nTo examine the eÔ¨Äects of component interchangeability, we adopt various base language models and similar-\\nity scoring mechanisms. For instance, if ùê∏ùê∏denotes the embedding function, and ùë†ùë† and ùë°ùë° represent the input and target \\ntext sequences, respectively, their vector representations would be ùêØùêØùë†ùë† = ùê∏ùê∏(ùë†ùë†) and ùêØùêØùë°ùë° = ùê∏ùê∏(ùë°ùë°). We employ cosine simi-\\nlarity as the basis for our similarity score, deÔ¨Åned by the formula: \\nsimilarity(ùêØùêØùë†ùë†, ùêØùêØùë°ùë°) = ùêØùêØùë†ùë† ‚ãÖùêØùêØùë°ùë°\\nÔøΩ|vùë†ùë†|ÔøΩ ÔøΩ|vùë°ùë°|ÔøΩ \\nHere, ‚ãÖ denotes the dot product between the two vectors, and ÔøΩ|‚ãÖ|ÔøΩ denotes the Euclidean norm. This score quantiÔ¨Åes \\nthe closeness of the semantic meaning represented by the vectors, with a value of 1 indicating identical directionality \\nand thus, maximal similarity.  \\nVolume 12 Issue 4 (2023) \\nISSN: 2167-1907\\nwww.JSR.org/hs\\n3\"),\n",
       " Document(metadata={'producer': 'Adobe PDF Library 23.8.53', 'creator': 'Acrobat PDFMaker 23 for Word', 'creationdate': '2024-02-21T12:54:50-06:00', 'author': 'JOFSR', 'comments': '', 'company': '', 'keywords': '', 'moddate': '2024-02-21T12:55:38-06:00', 'sourcemodified': 'D:20240221185350', 'subject': '', 'title': '', 'source': '..\\\\data\\\\pdf\\\\A retrieval-augmented generation based large language model benchmarked on a novel dataset.pdf', 'total_pages': 9, 'page': 3, 'page_label': '4', 'source_file': 'A retrieval-augmented generation based large language model benchmarked on a novel dataset.pdf', 'file_type': 'pdf'}, page_content=\"The experiment tests diÔ¨Äerent conÔ¨Ågurations by substituting ùê∏ùê∏ with embedding functions from various mod-\\nels (e.g., GPT, Palm), allowing us to discern the impact of the embedding layer on the Ô¨Ånal similarity score. By com-\\nparing the performance of diÔ¨Äerent  ùê∏ùê∏ choices, we can identify which embeddings yield the most semantically rich \\nrepresentations for our unique dataset. \\n \\nExperiment Setup \\n \\nThe experiment commences with the training of the language models using our unique dataset. For the training phase, \\nwe deÔ¨Åne the following: \\n‚Ñí: The base language model, which can be either GPT or Palm. \\nùíüùíü: The training dataset, consisting of pairs  (ùëûùëûùëñùëñ, ùëéùëéùëñùëñ) where ùëûùëûùëñùëñ is a query from the dataset and ùëéùëéùëñùëñ is the corresponding \\nanswer. \\nThe language model ‚Ñí is Ô¨Åne-tuned on ùíüùíü, optimizing the weights to minimize the loss function, typically a \\ncross-entropy loss between the predicted and actual answers.  \\nFollowing training, the question-answering process involves feeding a new query ùëûùëû‚Ä≤ to the trained model and retrieving \\nthe answer ùëéùëé‚Ä≤. This answer is then compared to a predeÔ¨Åned list of correct answers using the similarity score, which is \\nfundamental to evaluating the model's performance. \\n \\nBenchmarking and Evaluation \\n \\nThe evaluation metric for our experiment is based on the similarity scores between the generated responses and a set \\nof reference answers. Let ùê¥ùê¥ = {ùëéùëé1\\n‚Ä≤, ùëéùëé2\\n‚Ä≤ , ‚Ä¶ , ùëéùëéùëõùëõ\\n‚Ä≤ } be the set of answers generated by the model, and ùê¥ùê¥ref = {ùëéùëé1\\n‚àó, ùëéùëé2\\n‚àó, ‚Ä¶ , ùëéùëéùëõùëõ\\n‚àó} \\nbe the set of reference answers. We deÔ¨Åne the average similarity score as follows: \\nScoreavg = 1\\nn ÔøΩsimilarity(ùêØùêØùëéùëéùëñùëñ\\n‚Ä≤, ùêØùêØùëéùëéùëñùëñ\\n‚àó)\\nùëõùëõ\\nùëñùëñ=1\\n \\nThis average score acts as the primary benchmark for comparing diÔ¨Äerent model conÔ¨Ågurations. We systematically \\nrecord the scores across various combinations of language models and similarity scoring mechanisms to assess which \\nconÔ¨Ågurations yield the highest average similarity, indicating the most eÔ¨Äective model setup for our dataset. \\nAdditionally, we account for the presence or absence of context in the model's training and response genera-\\ntion. This is critical, as the presence of context has been shown to signiÔ¨Åcantly inÔ¨Çuence model performance, particu-\\nlarly in the domain of indigenous knowledge and biodiversity, where context provides essential background information \\nthat can drastically aÔ¨Äect the meaning and relevance of a response. \\nThrough this meticulous experimental setup, we aim to illuminate the intricate dynamics between diÔ¨Äerent \\ncomponents of the RAG framework and their collective impact on the model's ability to accurately replicate and convey \\nthe richness of the Amazon Rainforest's cultural and ecological knowledge. \\n \\nPre-Experiment Performance Expectations and Discussion \\n \\nIn the landscape of varying conÔ¨Ågurations, we hypothesize that certain setups will yield higher average similarity \\nscores than others, indicative of more nuanced and accurate language generation. Particularly, we expect that: \\nThe similarity scores for models trained with contextual data ùêØùêØcontext will surpass those trained without, due to the \\nenriched understanding and background the model has of the subject matter. \\nWhen aligning models with their native embeddings (e.g., GPT with OpenAI Embed, Palm with Palm Em-\\nbed), the semantic vector representations (ùêØùêØùë†ùë†, ùêØùêØùë°ùë°) should align more closely, thus producing higher similarity scores. \\nThe modular nature of the RAG setup will reveal that certain combinations of base language models and similarity \\nscoring mechanisms are more eÔ¨Äective than others, depending on whether context is included or not. \\nVolume 12 Issue 4 (2023) \\nISSN: 2167-1907\\nwww.JSR.org/hs\\n4\"),\n",
       " Document(metadata={'producer': 'Adobe PDF Library 23.8.53', 'creator': 'Acrobat PDFMaker 23 for Word', 'creationdate': '2024-02-21T12:54:50-06:00', 'author': 'JOFSR', 'comments': '', 'company': '', 'keywords': '', 'moddate': '2024-02-21T12:55:38-06:00', 'sourcemodified': 'D:20240221185350', 'subject': '', 'title': '', 'source': '..\\\\data\\\\pdf\\\\A retrieval-augmented generation based large language model benchmarked on a novel dataset.pdf', 'total_pages': 9, 'page': 4, 'page_label': '5', 'source_file': 'A retrieval-augmented generation based large language model benchmarked on a novel dataset.pdf', 'file_type': 'pdf'}, page_content=\"We denote the expected performance increase due to context as  ‚àÜcontext, and the alignment of native embeddings as \\n‚àÜnative. Mathematically, we can represent our hypothesis as: \\nScoreavg,context > Scoreavg,no context + ‚àÜcontext \\nScoreavg,native > Scoreavg,non‚àínative + ‚àÜnative \\nThese hypotheses will be tested through a series of experiments, allowing us to determine the optimal model conÔ¨Ågu-\\nration for processing and generating responses reÔ¨Çective of the Amazon Rainforest dataset. \\n \\nPotential Implications for LLMs \\n \\nThe results of this experiment are expected to have signiÔ¨Åcant implications for the development and Ô¨Åne -tuning of \\nLarge Language Models (LLMs). By identifying the most eÔ¨Äective conÔ¨Ågurations, we can oÔ¨Äer insights into the adapt-\\nability of these models to specialized datasets, which is crucial for applic ations that require a high degree of cultural \\nand contextual sensitivity. \\nMoreover, the experiment is poised to challenge the prevailing approach to LLM training and Ô¨Åne -tuning, \\nwhich often relies on static, one-size-Ô¨Åts-all models. Our Ô¨Åndings could suggest a shift towards a more dynamic, com-\\nponent-based approach, allowing for greater Ô¨Çexibility and precision in model performance across diverse domains. \\nThe potential success of the RAG framework in this context may also pave the way for more granular im-\\nprovements in LLMs, beyond the standard metrics of accuracy and Ô¨Çuency. It may, for instance, enhance the models' \\nability to engage with and preserve less-represented languages and dialects, fostering greater inclusivity and diversity \\nin the realm of natural language processing. \\n \\nImplications for Indigenous Knowledge Preservation \\n \\nThe signiÔ¨Åcance of our experiment extends beyond the technical accomplishments within the Ô¨Åeld of natural language \\nprocessing. It serves as a testament to the power of advanced computational techniques in preserving the rich tapestry \\nof human culture, particularly the imperiled knowledge of the Amazon Rainforest's indigenous peoples. By success-\\nfully training a language model to accurately reÔ¨Çect and communicate this knowledge, we not only preserve it for future \\ngenerations but also validate the importance of linguistic and cultural diversity in our global ecosystem. \\nThis experiment, should it succeed, will demonstrate a practical application of LLMs in the service of cultural \\npreservation. It emphasizes the role that technology can play in safeguarding intangible heritage, a mission that aligns \\nwith the broader objectives of UNESCO's Intangible Cultural Heritage initiatives. It serves as a model for how com-\\nmunities around the world can leverage technology to protect and share their unique cultural identities and knowledge \\nsystems. \\n \\nAdvancements in RAG Framework \\n \\nFrom a methodological standpoint, our experiment is poised to contribute to the advancement of the RAG framework \\nwithin the realm of AI language models. By dissecting the RAG components and examining their interplay, we will \\ngain insights into the mechanics of modular design in language models, oÔ¨Äering a blueprint for future research and \\ndevelopment. \\nThe outcomes of this experiment could lead to the evolution of RAG into a more nuanced and adaptable \\nframework, one that can be customized for specialized datasets and applications. This adaptability is critical as the \\ndemand for LLMs expands into increasingly varied and complex domains, from legal and medical to historical and \\nanthropological. \\nFurthermore, the experiment's focus on modularity could inspire a new wave of research into component -\\nbased architectures for LLMs. Such architectures may provide a more sustainable and eÔ¨Écient pathway to model im-\\nprovement, as opposed to the computationally intensive process of training large models from scratch. \\nVolume 12 Issue 4 (2023) \\nISSN: 2167-1907\\nwww.JSR.org/hs\\n5\"),\n",
       " Document(metadata={'producer': 'Adobe PDF Library 23.8.53', 'creator': 'Acrobat PDFMaker 23 for Word', 'creationdate': '2024-02-21T12:54:50-06:00', 'author': 'JOFSR', 'comments': '', 'company': '', 'keywords': '', 'moddate': '2024-02-21T12:55:38-06:00', 'sourcemodified': 'D:20240221185350', 'subject': '', 'title': '', 'source': '..\\\\data\\\\pdf\\\\A retrieval-augmented generation based large language model benchmarked on a novel dataset.pdf', 'total_pages': 9, 'page': 5, 'page_label': '6', 'source_file': 'A retrieval-augmented generation based large language model benchmarked on a novel dataset.pdf', 'file_type': 'pdf'}, page_content='In conclusion, the proposed experiment holds the potential to make signiÔ¨Åcant contributions to both the Ô¨Åeld of AI and \\nthe preservation of human cultural heritage. The insights gained could lead to a more inclusive and representative \\nfuture for LLMs, where the voices of all communities can be heard and understood. \\n \\n \\n \\n \\nFi\\ngure 1. Venn Diagram of Data Sources for RAG. This Ô¨Ågure represents a venn diagram of 3 sources of information \\n(google search results, OpenAI/Palm, proprietary data collected by the author) combined in order to create the ‚Äúout-\\nputted answer.‚Äù \\n \\n \\nFigure 2. Executive Diagram of Proposed RAG. This diagram outlines the various steps and procedures of the RAG \\nalgorithm from the input of the ‚Äúuser question‚Äù to the ‚Äúoutputted answer of the user question.‚Äù \\n \\nVolume 12 Issue 4 (2023) \\nISSN: 2167-1907\\nwww.JSR.org/hs\\n6'),\n",
       " Document(metadata={'producer': 'Adobe PDF Library 23.8.53', 'creator': 'Acrobat PDFMaker 23 for Word', 'creationdate': '2024-02-21T12:54:50-06:00', 'author': 'JOFSR', 'comments': '', 'company': '', 'keywords': '', 'moddate': '2024-02-21T12:55:38-06:00', 'sourcemodified': 'D:20240221185350', 'subject': '', 'title': '', 'source': '..\\\\data\\\\pdf\\\\A retrieval-augmented generation based large language model benchmarked on a novel dataset.pdf', 'total_pages': 9, 'page': 6, 'page_label': '7', 'source_file': 'A retrieval-augmented generation based large language model benchmarked on a novel dataset.pdf', 'file_type': 'pdf'}, page_content='Experimental Results and Discussion \\n \\nThe purpose of this section is to lay down the diÔ¨Äerent steps and customizations used within our experiment in order \\nto demonstrate the conclusive results of this experiment to the reader; our experiment using a RAG methodology \\naccurately shows how each component of a LLM positively or negatively aÔ¨Äects the accuracy of the outcome itself. \\nIn the initial world of LLM, in order to incrementally increase its performance engineers of these models would have \\nto Ô¨Åne tune them then retrain which took immense amounts of power and large amounts of null results. However, now \\nas they become more and more complex to tune models like OpenAI‚Äôs GPT and Google‚Äôs Bard have been plateauing \\nperformance wise. \\n \\nTab\\nle 1. Experimental Results. This table represents the various diÔ¨Äerent combinations of LLM components with \\nrespect to the average similarity score they each produced. \\n \\n Context LLM Embed for Similarity Score \\n Yes No GPT Palm OpenAI Embedding Palm Embedding Score \\n1   x x   x   0.75 \\n2 x  x   x 0.92 \\n3 x   x  x 0.93 \\n4  x  x  x 0.88 \\n5 x  x  x  0.997 \\n6 x   x x  0.996 \\n7  x  x x  0.91 \\n8   x x     x 0.897 \\n \\nThis paper produces a new solution to the slowing improvement of LLM in the form of RAG, a way to com-\\nponentize the models and break them down into smaller sections. This allows the user to add certain parts / combina-\\ntions to test the performance of those then to substitute diÔ¨Äerent modules in to see which leads to the largest perfor-\\nmance increase over the other. These customizable steps allow you to see minute diÔ¨Äerences in performance that slowly \\ntuning a model couldn‚Äôt have shown you previously. This is a novel way to approach the tuning of LLM and will only \\nserve to increase their accuracy as time moves forward. \\nAnother major component of our RAG methodology is the ability to switch out which embedding layer you \\nuse. The standard embedding (OpenAI) or Palm‚Äôs embed. When choosing between both of those some tradeoÔ¨Äs are \\nmade. \\nWhen using no context, Palm‚Äôs embedding layer seems to perform much better across the board, allowing for \\na much higher average similarity score, however this drastically shifts when given context as now OpenAI‚Äôs embedding \\nlayer performs much more soundly. The evidence for these claims is discussed later in this paper. \\nAdditionally, another beneÔ¨Åcial feature of the RAG optimization and breakdown style is the ability to cus-\\ntomize which similarity score the LLM uses to decide which answer to base its response oÔ¨Ä from the Q-A list. To go \\ninto further detail, the code when prompted with a user question compares the user question to the Q-A list and reorders \\nthe list based oÔ¨Ä highest similarity score to lowest, this allows the LLM to select the top 2 -3 answers to the highest \\nranked questions and continue generating its own response from there. \\nThe Ô¨Årst choice of similarity score was STS OpenAI Score while the second was STS Palm Score. In terms \\nof the data when GPT (for the purposes of precision all of the following results include context) and STS OpenAI were \\ncombined, you got an average similarity score of 0.997. If you instead pair this with Palm STS Score instead, the \\naverage score drops to 0.92, a 0.077 decrease in performance. A similar eÔ¨Äect when using Palm with Palm STS and \\nPalm with OpenAI STS (0.996 versus 0.93, respectively). This data demonstrates that both Palm and OpenAI are able \\nVolume 12 Issue 4 (2023) \\nISSN: 2167-1907\\nwww.JSR.org/hs\\n7'),\n",
       " Document(metadata={'producer': 'Adobe PDF Library 23.8.53', 'creator': 'Acrobat PDFMaker 23 for Word', 'creationdate': '2024-02-21T12:54:50-06:00', 'author': 'JOFSR', 'comments': '', 'company': '', 'keywords': '', 'moddate': '2024-02-21T12:55:38-06:00', 'sourcemodified': 'D:20240221185350', 'subject': '', 'title': '', 'source': '..\\\\data\\\\pdf\\\\A retrieval-augmented generation based large language model benchmarked on a novel dataset.pdf', 'total_pages': 9, 'page': 7, 'page_label': '8', 'source_file': 'A retrieval-augmented generation based large language model benchmarked on a novel dataset.pdf', 'file_type': 'pdf'}, page_content=\"to reach very high accuracy levels when paired with a similarity score calculated from the same program (this means \\nGPT worked better with OpenAI STS Score and that Palm worked better with Palm STS Score). What is also interest-\\ning to note is that although Palm produced a 0.001 lower performance than GPT it seemed to be more Ô¨Çexible, working \\nbetter with its competitor (OpenAI  STS Score + Palm produced 0.93) than the GPT with its competitor (Palm STS \\nScore + GPT produced 0.92). \\nTo recap on the experimental setup, this code uses an interchangeable piece of a LLM so you can swap or \\nreplace things like the embedding layer used, the similarity score used, and the base language model used, also whether \\nit was given context from the Q-A database or not. This is a novel and important way to be able to break down LLM \\nand the data collected speaks a lot to the importance of each aspect of an LLM. \\nIn terms of expected results, two things were noticed, Ô¨Årst that Palm had a slightly lower performance than \\nGPT (0.996 versus 0.997 respectively) on their top runs, however, there was also some contradictory data as it seemed \\nthat Palm worked signiÔ¨Åcantly better when given no context to work with compared to GPT, Palm produced an average \\nscore of 0.88 and 0.91 when given no context while GPT produced an average score of 0.75 and 0.897. Although GPT \\nmay perform much better when given context, Palm seems to beat it out just given its own proprietary dataset (no \\ncontext). \\nSome unexpected results occurred with pairing GPT and Palm with their opposite embeds, for example, pair-\\ning Palm with OpenAI embed. While on paper it makes sense that Palm would work better with Palm embed, it actually \\nperformed better when paired with OpenAI‚Äôs embed. 0.91 (with OpenAI embed) versus 0.88 (Palm embed)‚Äînote that \\nthis is without context given. A similar eÔ¨Äect was noticed going the other way around as well, without context, GPT  \\nperformed much better with Palm embedding layer than with its own OpenAI embedding layer (0.897 versus 0.75 \\nrespectively). This data shows how Palm embedding layer tends to perform much better given no context when com-\\npared to OpenAI ‚Äôs embed. Similarly, to explored above, it is the opposite when given context, however. OpenAI‚Äôs \\nembedding layer performs a bit better when given context across the board than Palm embed. \\nMost notably from this experiment was two realizations. First, that Palm Embedding layer tends to work much \\nbetter when just given its own proprietary dataset (and no context), when compared to OpenAI‚Äôs embed. Additionally, \\nwhen given context, the playing Ô¨Åeld switches: Palm tends to perform much worse when given context when compared \\nto OpenAI. Lastly, it is important to note that a combination of Palm/GPT with OpenAI‚Äôs embedding layer and context \\nyielded extremely accurate results when its similarity scores were averaged. \\n \\nConclusion \\n \\nBuilding upon the foundation laid by our initial Ô¨Åndings, it is paramount to recognize the exceptional performance of \\nthe Palm model when utilizing its proprietary dataset in conjunction with the Palm embed. This speciÔ¨Åcity in data and \\ntechnology synchronization has shown that Palm outshines OpenAI in terms of model accuracy in a context-free envi-\\nronment. However, the landscape shifts when contextual data is integrated. In such scenarios, the combination of GPT \\nwith its native OpenAI embedding layer excels, leveraging the additional context to produce responses of remarkable \\naccuracy that resonate with the cultural and ecological nuances of the Amazon. \\nThis pivot in performance based on context underscores the signiÔ¨Åcance of tailored datasets and embedding \\nmechanisms in the optimization of Large Language Models (LLMs). The adaptability of the Retrieval -Augmented \\nGeneration (RAG) framework emerges as a cornerstone for future enhancements in LLMs. By enabling the seamless \\ninterchange of model components, RAG presents an evolutionary leap in the Ô¨Åne-tuning of language models, catering \\nto the intricate demands of culturally rich and contextually complex datasets. \\nIn light of these advancements, our research signiÔ¨Åes a pivotal moment for LLMs. The evidence suggests that \\nwhen models are Ô¨Ånely tuned with an awareness of the dataset's inherent context and the corresponding embedding \\nlayers, they reach new heights of linguistic precision. Therefore, the path forward for LLMs lies in embracing the \\nmodular and contextually aware RAG framework, which promises to reÔ¨Åne the capabilities of language models to an \\nunprecedented degree, ensuring the preservation and celebration of the world's diverse linguistic and cultural heritage. \\nVolume 12 Issue 4 (2023) \\nISSN: 2167-1907\\nwww.JSR.org/hs\\n8\"),\n",
       " Document(metadata={'producer': 'Adobe PDF Library 23.8.53', 'creator': 'Acrobat PDFMaker 23 for Word', 'creationdate': '2024-02-21T12:54:50-06:00', 'author': 'JOFSR', 'comments': '', 'company': '', 'keywords': '', 'moddate': '2024-02-21T12:55:38-06:00', 'sourcemodified': 'D:20240221185350', 'subject': '', 'title': '', 'source': '..\\\\data\\\\pdf\\\\A retrieval-augmented generation based large language model benchmarked on a novel dataset.pdf', 'total_pages': 9, 'page': 8, 'page_label': '9', 'source_file': 'A retrieval-augmented generation based large language model benchmarked on a novel dataset.pdf', 'file_type': 'pdf'}, page_content='Limitations \\n \\nThe results of the ‚Äúoutputted answer‚Äù of this algorithm largely reÔ¨Çect the quality of the data. If the LLM is trained oÔ¨Ä \\nlow-quality data, then the answer will reÔ¨Çect this bias. The results of the experiment will Ô¨Çuctuate with diÔ¨Äerent results \\nshould a diÔ¨Äerent dataset be used to train the LLM.  \\nIn the future there is a lot of potential to expand on this research by breaking down the RAG algorithm into \\nev\\nen more separate components to further see the diÔ¨Äerences in average similarity score that adding or removing each \\ncomponent makes. \\n \\nRefer ences \\n \\nBahdanau, D. C. (2016). End-to-end attention-based large vocabulary speech recognition. 2016 IEEE international \\nconference on acoustics, speech and signal processing (ICASSP), 4945-4949. \\nDevlin, J. C. (2018). Bert: Pre-training of deep bidirectional transformers for language understanding. arXiv preprint, \\narXiv:1810.04805. \\nLewis, P. P. (2020). Retrieval-augmented generation for knowledge-intensive nlp tasks. Advances in Neural \\nInformation Processing Systems, 9459-9474. \\nRadford, A. N. (2018). Improving language understanding by generative pre-training. OpenAI. \\nSiriwardhana, S. W. (2023). Improving the domain adaptation of retrieval augmented generation (RAG) models for \\nopen domain question answering. Transactions of the Association for Computational Linguistics, 1-17. \\nVaswani, A. S. (2017). Attention is all you need. Advances in neural information processing systems. \\nYu, W. (2022). Retrieval-augmented generation across heterogeneous knowledge. Proceedings of the 2022 \\nConference of the North American Chapter of the Association for Computational Linguistics: Human Language \\nTechnologies: Student Research Workshop, 52-58. \\n \\n \\n  \\nVolume 12 Issue 4 (2023) \\nISSN: 2167-1907\\nwww.JSR.org/hs\\n9')]"
      ]
     },
     "execution_count": 8,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "all_pdf_documents"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "c73f8b01",
   "metadata": {},
   "source": [
    "### Text splitting get into chunks"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "fab6b6cc",
   "metadata": {},
   "outputs": [],
   "source": [
    "def split_documents(documents,chunk_size=1000,chunk_overlap=200):\n",
    "    \"\"\"Split documents into smaller chunks for better RAG performance\"\"\"\n",
    "    \n",
    "    text_splitter = RecursiveCharacterTextSplitter(\n",
    "        chunk_size=chunk_size,\n",
    "        chunk_overlap=chunk_overlap,\n",
    "        length_function=len,\n",
    "        separators=[\"\\n\\n\", \"\\n\", \" \", \"\"]\n",
    "    )\n",
    "    split_docs = text_splitter.split_documents(documents)\n",
    "    print(f\"Split {len(documents)} documents into {len(split_docs)} chunks\")\n",
    "    \n",
    "    # Show example of a chunk\n",
    "    if split_docs:\n",
    "        print(f\"\\nExample chunk:\")\n",
    "        print(f\"Content: {split_docs[0].page_content[:200]}...\")\n",
    "        print(f\"Metadata: {split_docs[0].metadata}\")\n",
    "    \n",
    "    return split_docs\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "c7b75a72",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Split 9 documents into 41 chunks\n",
      "\n",
      "Example chunk:\n",
      "Content: A Retrieval-Augmented Generation Based Large \n",
      "Language Model Benchmarked on a Novel Dataset \n",
      "Kieran Pichai \n",
      "Menlo School \n",
      "AB\n",
      "STRACT \n",
      "The evolution of natural language processing has seen marked advanc...\n",
      "Metadata: {'producer': 'Adobe PDF Library 23.8.53', 'creator': 'Acrobat PDFMaker 23 for Word', 'creationdate': '2024-02-21T12:54:50-06:00', 'author': 'JOFSR', 'comments': '', 'company': '', 'keywords': '', 'moddate': '2024-02-21T12:55:38-06:00', 'sourcemodified': 'D:20240221185350', 'subject': '', 'title': '', 'source': '..\\\\data\\\\pdf\\\\A retrieval-augmented generation based large language model benchmarked on a novel dataset.pdf', 'total_pages': 9, 'page': 0, 'page_label': '1', 'source_file': 'A retrieval-augmented generation based large language model benchmarked on a novel dataset.pdf', 'file_type': 'pdf'}\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "[Document(metadata={'producer': 'Adobe PDF Library 23.8.53', 'creator': 'Acrobat PDFMaker 23 for Word', 'creationdate': '2024-02-21T12:54:50-06:00', 'author': 'JOFSR', 'comments': '', 'company': '', 'keywords': '', 'moddate': '2024-02-21T12:55:38-06:00', 'sourcemodified': 'D:20240221185350', 'subject': '', 'title': '', 'source': '..\\\\data\\\\pdf\\\\A retrieval-augmented generation based large language model benchmarked on a novel dataset.pdf', 'total_pages': 9, 'page': 0, 'page_label': '1', 'source_file': 'A retrieval-augmented generation based large language model benchmarked on a novel dataset.pdf', 'file_type': 'pdf'}, page_content='A Retrieval-Augmented Generation Based Large \\nLanguage Model Benchmarked on a Novel Dataset \\nKieran Pichai \\nMenlo School \\nAB\\nSTRACT \\nThe evolution of natural language processing has seen marked advancements, particularly with the advent of models \\nlike BERT, Transformers, and GPT variants, with recent additions like GPT  and Bard. This paper investigates the \\nRetrieval-Augmented Generation (RAG) framework, providing insights into its modular design and the impact of its \\nconstituent modules on performance. Leveraging a unique dataset from Amazon Rainforest natives and biologists, our \\nresearch demonstrates the signiÔ¨Åcance of preserving indigenous cultures and biodiversity. The experiment employs a \\ncustomizable RAG methodology, allowing for the interchangeability of various components, such as the base language \\nmodel and similarity score tools. Findings indicate that while GPT performs slightly better when given context, Palm'),\n",
       " Document(metadata={'producer': 'Adobe PDF Library 23.8.53', 'creator': 'Acrobat PDFMaker 23 for Word', 'creationdate': '2024-02-21T12:54:50-06:00', 'author': 'JOFSR', 'comments': '', 'company': '', 'keywords': '', 'moddate': '2024-02-21T12:55:38-06:00', 'sourcemodified': 'D:20240221185350', 'subject': '', 'title': '', 'source': '..\\\\data\\\\pdf\\\\A retrieval-augmented generation based large language model benchmarked on a novel dataset.pdf', 'total_pages': 9, 'page': 0, 'page_label': '1', 'source_file': 'A retrieval-augmented generation based large language model benchmarked on a novel dataset.pdf', 'file_type': 'pdf'}, page_content='model and similarity score tools. Findings indicate that while GPT performs slightly better when given context, Palm \\nexhibits superior performance without context. The results also suggest that models tend to perform optimally when \\npaired with similarity scores from their native platforms. Conclusively, our approach show cases the potential of a \\nmodular RAG design in optimizing language models, presenting it as a more advantageous strategy compared to tra-\\nditional Ô¨Åne-tuning of large language models. \\nIntroduction \\nThe evolution of natural language processing models has seen signiÔ¨Åcant strides from rule -based approaches in the \\nearly stages of language understanding, eventually leading to the advent of neural networks. However, the full potential \\nof these neural netw orks awaited the computational infrastructure to catch up. The pivotal moment arrived with the \\nemergence of neural machine translation (NMT), exempliÔ¨Åed by Google Translate, which marked a turning point in'),\n",
       " Document(metadata={'producer': 'Adobe PDF Library 23.8.53', 'creator': 'Acrobat PDFMaker 23 for Word', 'creationdate': '2024-02-21T12:54:50-06:00', 'author': 'JOFSR', 'comments': '', 'company': '', 'keywords': '', 'moddate': '2024-02-21T12:55:38-06:00', 'sourcemodified': 'D:20240221185350', 'subject': '', 'title': '', 'source': '..\\\\data\\\\pdf\\\\A retrieval-augmented generation based large language model benchmarked on a novel dataset.pdf', 'total_pages': 9, 'page': 0, 'page_label': '1', 'source_file': 'A retrieval-augmented generation based large language model benchmarked on a novel dataset.pdf', 'file_type': 'pdf'}, page_content='emergence of neural machine translation (NMT), exempliÔ¨Åed by Google Translate, which marked a turning point in \\nmachine language comprehension (Bahdanau, 2016). Subsequently, a plethora of advanced models, including BERT, \\nTransformers, GPT-2, and GPT-3, have emerged, driving the Ô¨Åeld forward. Recent notable additions to this landscape \\nare models like GPT and Bard (Devlin, 2018) (Vaswani, 2017) (Radford, 2018). While Ô¨Åne-tuning such models has \\nproven to be a challenging endeavor, it has become evident that Retrieval-Augmented Generation (RAG) oÔ¨Äers a prom-\\nising alternative (Lewis, 2020) (Siriwardhana, 2023) (Yu, 2022). \\nCuriously, little attention has been devoted to dissecting the individual components of RAG and their respec-\\ntive impacts on overall performance. In response to this gap, our paper undertakes a comprehensive investigation of \\nthe RAG framework and embarks on the design of RAG models from the ground up, with a focus on the modularity'),\n",
       " Document(metadata={'producer': 'Adobe PDF Library 23.8.53', 'creator': 'Acrobat PDFMaker 23 for Word', 'creationdate': '2024-02-21T12:54:50-06:00', 'author': 'JOFSR', 'comments': '', 'company': '', 'keywords': '', 'moddate': '2024-02-21T12:55:38-06:00', 'sourcemodified': 'D:20240221185350', 'subject': '', 'title': '', 'source': '..\\\\data\\\\pdf\\\\A retrieval-augmented generation based large language model benchmarked on a novel dataset.pdf', 'total_pages': 9, 'page': 0, 'page_label': '1', 'source_file': 'A retrieval-augmented generation based large language model benchmarked on a novel dataset.pdf', 'file_type': 'pdf'}, page_content='the RAG framework and embarks on the design of RAG models from the ground up, with a focus on the modularity \\nand replaceability of its constituent modules. This research seeks to contribute to a deeper understanding of the mech-\\nanisms underlying RAG and its potential for enhancing natural language understanding and generation. These Large \\nLanguage Models (LLMs) exhibit a remarkable proÔ¨Åciency in replicating human language styles, achieving a level of \\nlinguistic verisimilitude that borders on the impeccable. In light of these capabilities, it is prudent to delve into the \\nprofound signiÔ¨Åcance of the Amazon rainforest, which equates to the importance of any ethnically or racially diverse \\nnation across the globe. Within the vast expanse of the Amazon, an intricate tapestry of life unfolds, where millions \\nof distinct species intermingle. Each of these species, as rare as the other, holds a unique and intrinsic value to the'),\n",
       " Document(metadata={'producer': 'Adobe PDF Library 23.8.53', 'creator': 'Acrobat PDFMaker 23 for Word', 'creationdate': '2024-02-21T12:54:50-06:00', 'author': 'JOFSR', 'comments': '', 'company': '', 'keywords': '', 'moddate': '2024-02-21T12:55:38-06:00', 'sourcemodified': 'D:20240221185350', 'subject': '', 'title': '', 'source': '..\\\\data\\\\pdf\\\\A retrieval-augmented generation based large language model benchmarked on a novel dataset.pdf', 'total_pages': 9, 'page': 0, 'page_label': '1', 'source_file': 'A retrieval-augmented generation based large language model benchmarked on a novel dataset.pdf', 'file_type': 'pdf'}, page_content='of distinct species intermingle. Each of these species, as rare as the other, holds a unique and intrinsic value to the \\nindigenous populations who have made this ecosystem their home. The Amazon rainforest is not only a cradle of \\nbiological diversity but also a sanctuary for an array of religions and cultures, many of which teeter on the brink of \\nVolume 12 Issue 4 (2023) \\nISSN: 2167-1907\\nwww.JSR.org/hs\\n1'),\n",
       " Document(metadata={'producer': 'Adobe PDF Library 23.8.53', 'creator': 'Acrobat PDFMaker 23 for Word', 'creationdate': '2024-02-21T12:54:50-06:00', 'author': 'JOFSR', 'comments': '', 'company': '', 'keywords': '', 'moddate': '2024-02-21T12:55:38-06:00', 'sourcemodified': 'D:20240221185350', 'subject': '', 'title': '', 'source': '..\\\\data\\\\pdf\\\\A retrieval-augmented generation based large language model benchmarked on a novel dataset.pdf', 'total_pages': 9, 'page': 1, 'page_label': '2', 'source_file': 'A retrieval-augmented generation based large language model benchmarked on a novel dataset.pdf', 'file_type': 'pdf'}, page_content='oblivion. Preserving the Amazon is not merely an environmental imperative; it is an act of justice to the indigenous \\ncommunities whose ancestral lands are enshrined within its boundaries. It is a call to safeguard the memories of the \\nland, the traditions that have evolved within its embrace, and the very essence of their cultures. However, certain \\nregions of the Amazon remain shrouded in obscurity, their Ô¨Çora and fauna so rare that reliable and readily available \\ninformation is conspicuously lacking in the vast repository of knowledge available on the internet. In this context, \\nadvanced LLMs play an instrumental role in addressing this deÔ¨Åcit by facilitating the dissemination of indigenous \\nnarratives and thereby amplifying awareness and appreciation of the rich tapestry of beliefs, practices, and traditional \\nknowledge that these communities hold dear. They serve as a bridge connecting the indigenous Amazonian cultures'),\n",
       " Document(metadata={'producer': 'Adobe PDF Library 23.8.53', 'creator': 'Acrobat PDFMaker 23 for Word', 'creationdate': '2024-02-21T12:54:50-06:00', 'author': 'JOFSR', 'comments': '', 'company': '', 'keywords': '', 'moddate': '2024-02-21T12:55:38-06:00', 'sourcemodified': 'D:20240221185350', 'subject': '', 'title': '', 'source': '..\\\\data\\\\pdf\\\\A retrieval-augmented generation based large language model benchmarked on a novel dataset.pdf', 'total_pages': 9, 'page': 1, 'page_label': '2', 'source_file': 'A retrieval-augmented generation based large language model benchmarked on a novel dataset.pdf', 'file_type': 'pdf'}, page_content='knowledge that these communities hold dear. They serve as a bridge connecting the indigenous Amazonian cultures \\nwith the global community, emphasizing the paramount importance of preserving the cultural diversity interwoven \\nwithin this vast rainforest. In sum, the overarching mission of this endeavor is twofold: to document and educate the \\nWestern world about hitherto unknown cultures while concurrently ensuring the enduring preservation of these inval-\\nuable facets of human heritage and biodiversity. \\n \\nProposed Experiment \\n \\nBackground and Importance \\n \\nThe intrinsic value of indigenous knowledge, especially from regions as biodiverse and culturally rich as the Amazon \\nRainforest, cannot be overstated. This knowledge, passed down through generations, encompasses not only cultural \\nand religious beliefs but also practical insights into the local Ô¨Çora and fauna. As the modern world encroaches on these'),\n",
       " Document(metadata={'producer': 'Adobe PDF Library 23.8.53', 'creator': 'Acrobat PDFMaker 23 for Word', 'creationdate': '2024-02-21T12:54:50-06:00', 'author': 'JOFSR', 'comments': '', 'company': '', 'keywords': '', 'moddate': '2024-02-21T12:55:38-06:00', 'sourcemodified': 'D:20240221185350', 'subject': '', 'title': '', 'source': '..\\\\data\\\\pdf\\\\A retrieval-augmented generation based large language model benchmarked on a novel dataset.pdf', 'total_pages': 9, 'page': 1, 'page_label': '2', 'source_file': 'A retrieval-augmented generation based large language model benchmarked on a novel dataset.pdf', 'file_type': 'pdf'}, page_content='and religious beliefs but also practical insights into the local Ô¨Çora and fauna. As the modern world encroaches on these \\nlands, this wisdom is in peril of being lost forever. Recognizing this, our proposed experiment aims to employ a state-\\nof-the-art Retrieval- Augmented Generation (RAG) framework to capture and leverage this  vast, yet vulnerable, \\nknowledge base. \\nOur dataset, derived from interviews with Amazon Rainforest natives and biologists, is unparalleled in its \\ndepth and breadth. It includes detailed discussions on religious practices, cultural nuances, and the integral role of the \\nsurrounding ecosystem in the daily lives of these communities. This data is not just a scientiÔ¨Åc or anthropological \\nresource; it is a repository of living history and an urgent call to action for preservation eÔ¨Äorts. \\nBy integrating this unique dataset into the RAG framework, we anticipate not only the preservation of'),\n",
       " Document(metadata={'producer': 'Adobe PDF Library 23.8.53', 'creator': 'Acrobat PDFMaker 23 for Word', 'creationdate': '2024-02-21T12:54:50-06:00', 'author': 'JOFSR', 'comments': '', 'company': '', 'keywords': '', 'moddate': '2024-02-21T12:55:38-06:00', 'sourcemodified': 'D:20240221185350', 'subject': '', 'title': '', 'source': '..\\\\data\\\\pdf\\\\A retrieval-augmented generation based large language model benchmarked on a novel dataset.pdf', 'total_pages': 9, 'page': 1, 'page_label': '2', 'source_file': 'A retrieval-augmented generation based large language model benchmarked on a novel dataset.pdf', 'file_type': 'pdf'}, page_content='By integrating this unique dataset into the RAG framework, we anticipate not only the preservation of \\nknowledge but also the generation of responses that reÔ¨Çect the rich tapestry of Amazonian life. The experiment is \\ndesigned to evaluate how diÔ¨Äerent components within the RAG setup‚Äîsuch as base language models and similarity \\nscoring algorithms‚Äîcan be optimized to reÔ¨Çect the nuances captured within our dataset. In doing so, we aim to bridge \\nthe gap between advanced language models and the profound human insights found within the Amazon. \\nThe central objective of our experiment is twofold: to analyze the performance implications of modular design \\nwithin the RAG framework and to demonstrate the profound capability of such a system to preserve and communicate \\nthe wealth of indigenous knowledge. We hypothesize that a customizable RAG model will not only facilitate a deeper'),\n",
       " Document(metadata={'producer': 'Adobe PDF Library 23.8.53', 'creator': 'Acrobat PDFMaker 23 for Word', 'creationdate': '2024-02-21T12:54:50-06:00', 'author': 'JOFSR', 'comments': '', 'company': '', 'keywords': '', 'moddate': '2024-02-21T12:55:38-06:00', 'sourcemodified': 'D:20240221185350', 'subject': '', 'title': '', 'source': '..\\\\data\\\\pdf\\\\A retrieval-augmented generation based large language model benchmarked on a novel dataset.pdf', 'total_pages': 9, 'page': 1, 'page_label': '2', 'source_file': 'A retrieval-augmented generation based large language model benchmarked on a novel dataset.pdf', 'file_type': 'pdf'}, page_content='the wealth of indigenous knowledge. We hypothesize that a customizable RAG model will not only facilitate a deeper \\nunderstanding of the data but also allow us to Ô¨Åne -tune the system for optimal performance across diÔ¨Äerent conÔ¨Ågu-\\nrations. To achieve this, we will systematically explore the interchangeability of various RAG components. We will \\nassess diÔ¨Äerent base language models such as GPT and Palm and compare the eÔ¨Écacy of similarity scoring tools from \\ndiverse platforms. The experiment will rigorously test these combinations, identifying which synergies most eÔ¨Äectively \\ncapture the essence of the dataset. \\nThe ultimate goal is to showcase the potential of a modular RAG system in processing culturally signiÔ¨Åcant \\ninformation, paving the way for future applications that can beneÔ¨Åt from such tailored language models. We anticipate \\nthat our Ô¨Åndings will contribute signiÔ¨Åcantly to the Ô¨Åelds of computational linguistics and cultural preservation, demon-'),\n",
       " Document(metadata={'producer': 'Adobe PDF Library 23.8.53', 'creator': 'Acrobat PDFMaker 23 for Word', 'creationdate': '2024-02-21T12:54:50-06:00', 'author': 'JOFSR', 'comments': '', 'company': '', 'keywords': '', 'moddate': '2024-02-21T12:55:38-06:00', 'sourcemodified': 'D:20240221185350', 'subject': '', 'title': '', 'source': '..\\\\data\\\\pdf\\\\A retrieval-augmented generation based large language model benchmarked on a novel dataset.pdf', 'total_pages': 9, 'page': 1, 'page_label': '2', 'source_file': 'A retrieval-augmented generation based large language model benchmarked on a novel dataset.pdf', 'file_type': 'pdf'}, page_content='that our Ô¨Åndings will contribute signiÔ¨Åcantly to the Ô¨Åelds of computational linguistics and cultural preservation, demon-\\nstrating a novel approach to the application of large language models. \\n \\n \\n \\nVolume 12 Issue 4 (2023) \\nISSN: 2167-1907\\nwww.JSR.org/hs\\n2'),\n",
       " Document(metadata={'producer': 'Adobe PDF Library 23.8.53', 'creator': 'Acrobat PDFMaker 23 for Word', 'creationdate': '2024-02-21T12:54:50-06:00', 'author': 'JOFSR', 'comments': '', 'company': '', 'keywords': '', 'moddate': '2024-02-21T12:55:38-06:00', 'sourcemodified': 'D:20240221185350', 'subject': '', 'title': '', 'source': '..\\\\data\\\\pdf\\\\A retrieval-augmented generation based large language model benchmarked on a novel dataset.pdf', 'total_pages': 9, 'page': 2, 'page_label': '3', 'source_file': 'A retrieval-augmented generation based large language model benchmarked on a novel dataset.pdf', 'file_type': 'pdf'}, page_content='Source and Composition \\n \\nOur proprietary dataset stands as the cornerstone of this experiment. It is a rich compendium of verbal histories, inter-\\nviews, and ecological insights gathered from the indigenous peoples of the Amazon Rainforest, as well as from biolo-\\ngists and ecologists dedicated to studying this unique biome. The dataset is characterized by its diversity, comprising \\nnarratives that elucidate the intricate relationship between the natives and their environment, including the religious \\nand cultural signiÔ¨Åcance of plant and animal life. \\nThe data collection was an extensive process, where linguists and researchers engaged in deep conversations \\nwith the natives, recording their dialects, translating their stories, and documenting their knowledge of the ecological \\nsystem. Similarly, biologists contributed their decades of research on the Ô¨Çora and fauna, providing a scientiÔ¨Åc per-'),\n",
       " Document(metadata={'producer': 'Adobe PDF Library 23.8.53', 'creator': 'Acrobat PDFMaker 23 for Word', 'creationdate': '2024-02-21T12:54:50-06:00', 'author': 'JOFSR', 'comments': '', 'company': '', 'keywords': '', 'moddate': '2024-02-21T12:55:38-06:00', 'sourcemodified': 'D:20240221185350', 'subject': '', 'title': '', 'source': '..\\\\data\\\\pdf\\\\A retrieval-augmented generation based large language model benchmarked on a novel dataset.pdf', 'total_pages': 9, 'page': 2, 'page_label': '3', 'source_file': 'A retrieval-augmented generation based large language model benchmarked on a novel dataset.pdf', 'file_type': 'pdf'}, page_content=\"system. Similarly, biologists contributed their decades of research on the Ô¨Çora and fauna, providing a scientiÔ¨Åc per-\\nspective to the indigenous narratives. The data thus forms a conÔ¨Çuence of traditional wisdom and modern scientiÔ¨Åc \\nunderstanding, oÔ¨Äering a 360-degree view of the Amazon Rainforest's ecosystem. \\n \\nCultural and Environmental SigniÔ¨Åcance \\n \\nThe urgency of preserving indigenous knowledge is akin to conserving an endangered species. It is a race against time, \\nas globalization and environmental degradation threaten to erase unique cultures and the wisdom they hold. Our dataset \\nserves as a digital ark, a means to preserve and perpetuate the knowledge that has sustained the Amazon's communities \\nfor millennia. \\nThe environmental signiÔ¨Åcance of the Amazon Rainforest cannot be overstated‚Äîit is a keystone of global \\nbiodiversity. By documenting the intricate knowledge, the natives have of their environment, we are also chronicling\"),\n",
       " Document(metadata={'producer': 'Adobe PDF Library 23.8.53', 'creator': 'Acrobat PDFMaker 23 for Word', 'creationdate': '2024-02-21T12:54:50-06:00', 'author': 'JOFSR', 'comments': '', 'company': '', 'keywords': '', 'moddate': '2024-02-21T12:55:38-06:00', 'sourcemodified': 'D:20240221185350', 'subject': '', 'title': '', 'source': '..\\\\data\\\\pdf\\\\A retrieval-augmented generation based large language model benchmarked on a novel dataset.pdf', 'total_pages': 9, 'page': 2, 'page_label': '3', 'source_file': 'A retrieval-augmented generation based large language model benchmarked on a novel dataset.pdf', 'file_type': 'pdf'}, page_content=\"biodiversity. By documenting the intricate knowledge, the natives have of their environment, we are also chronicling \\nthe ecological interdependencies that are vital for the rainforest's survival. This dataset, therefore, is not just an aca-\\ndemic or technological asset; it is a critical record for environmental conservationists and policymakers. \\nThrough our experiment, we aim to amplify the voices of the Amazon's indigenous peoples, whose under-\\nstanding of their habitat is unmatched. By integrating their knowledge into the RAG framework, we hope to create a \\nmodel that not only responds with information but also with wisdom  that respects the interconnectedness of life and \\nculture. \\n \\nRetrieval-Augmented Generation Framework \\n \\nThe heart of our experiment lies in the Retrieval-Augmented Generation (RAG) framework, a sophisticated algorithm \\nthat enables the deconstruction of the language model into discrete, interchangeable components. This framework\"),\n",
       " Document(metadata={'producer': 'Adobe PDF Library 23.8.53', 'creator': 'Acrobat PDFMaker 23 for Word', 'creationdate': '2024-02-21T12:54:50-06:00', 'author': 'JOFSR', 'comments': '', 'company': '', 'keywords': '', 'moddate': '2024-02-21T12:55:38-06:00', 'sourcemodified': 'D:20240221185350', 'subject': '', 'title': '', 'source': '..\\\\data\\\\pdf\\\\A retrieval-augmented generation based large language model benchmarked on a novel dataset.pdf', 'total_pages': 9, 'page': 2, 'page_label': '3', 'source_file': 'A retrieval-augmented generation based large language model benchmarked on a novel dataset.pdf', 'file_type': 'pdf'}, page_content='that enables the deconstruction of the language model into discrete, interchangeable components. This framework \\nintegrates a retriever model t hat sources relevant context and a generator model that synthesizes the retrieved infor-\\nmation into coherent responses. \\nIn mathematical terms, given an input query ùëûùëû, the retriever model searches a knowledge base ùí¶ùí¶ and retrieves \\na set of relevant documents ùê∑ùê∑ = {ùëëùëë1, ùëëùëë2, ‚Ä¶ , ùëëùëëùëòùëò}. Each document ùëëùëë is represented as a vector ùêØùêØùëëùëë in a high-dimensional \\nspace, obtained from an embedding layer. This process transforms the raw text data into a structured form amenable \\nto computational manipulation. \\nTo examine the eÔ¨Äects of component interchangeability, we adopt various base language models and similar-\\nity scoring mechanisms. For instance, if ùê∏ùê∏denotes the embedding function, and ùë†ùë† and ùë°ùë° represent the input and target'),\n",
       " Document(metadata={'producer': 'Adobe PDF Library 23.8.53', 'creator': 'Acrobat PDFMaker 23 for Word', 'creationdate': '2024-02-21T12:54:50-06:00', 'author': 'JOFSR', 'comments': '', 'company': '', 'keywords': '', 'moddate': '2024-02-21T12:55:38-06:00', 'sourcemodified': 'D:20240221185350', 'subject': '', 'title': '', 'source': '..\\\\data\\\\pdf\\\\A retrieval-augmented generation based large language model benchmarked on a novel dataset.pdf', 'total_pages': 9, 'page': 2, 'page_label': '3', 'source_file': 'A retrieval-augmented generation based large language model benchmarked on a novel dataset.pdf', 'file_type': 'pdf'}, page_content='ity scoring mechanisms. For instance, if ùê∏ùê∏denotes the embedding function, and ùë†ùë† and ùë°ùë° represent the input and target \\ntext sequences, respectively, their vector representations would be ùêØùêØùë†ùë† = ùê∏ùê∏(ùë†ùë†) and ùêØùêØùë°ùë° = ùê∏ùê∏(ùë°ùë°). We employ cosine simi-\\nlarity as the basis for our similarity score, deÔ¨Åned by the formula: \\nsimilarity(ùêØùêØùë†ùë†, ùêØùêØùë°ùë°) = ùêØùêØùë†ùë† ‚ãÖùêØùêØùë°ùë°\\nÔøΩ|vùë†ùë†|ÔøΩ ÔøΩ|vùë°ùë°|ÔøΩ \\nHere, ‚ãÖ denotes the dot product between the two vectors, and ÔøΩ|‚ãÖ|ÔøΩ denotes the Euclidean norm. This score quantiÔ¨Åes \\nthe closeness of the semantic meaning represented by the vectors, with a value of 1 indicating identical directionality \\nand thus, maximal similarity.  \\nVolume 12 Issue 4 (2023) \\nISSN: 2167-1907\\nwww.JSR.org/hs\\n3'),\n",
       " Document(metadata={'producer': 'Adobe PDF Library 23.8.53', 'creator': 'Acrobat PDFMaker 23 for Word', 'creationdate': '2024-02-21T12:54:50-06:00', 'author': 'JOFSR', 'comments': '', 'company': '', 'keywords': '', 'moddate': '2024-02-21T12:55:38-06:00', 'sourcemodified': 'D:20240221185350', 'subject': '', 'title': '', 'source': '..\\\\data\\\\pdf\\\\A retrieval-augmented generation based large language model benchmarked on a novel dataset.pdf', 'total_pages': 9, 'page': 3, 'page_label': '4', 'source_file': 'A retrieval-augmented generation based large language model benchmarked on a novel dataset.pdf', 'file_type': 'pdf'}, page_content='The experiment tests diÔ¨Äerent conÔ¨Ågurations by substituting ùê∏ùê∏ with embedding functions from various mod-\\nels (e.g., GPT, Palm), allowing us to discern the impact of the embedding layer on the Ô¨Ånal similarity score. By com-\\nparing the performance of diÔ¨Äerent  ùê∏ùê∏ choices, we can identify which embeddings yield the most semantically rich \\nrepresentations for our unique dataset. \\n \\nExperiment Setup \\n \\nThe experiment commences with the training of the language models using our unique dataset. For the training phase, \\nwe deÔ¨Åne the following: \\n‚Ñí: The base language model, which can be either GPT or Palm. \\nùíüùíü: The training dataset, consisting of pairs  (ùëûùëûùëñùëñ, ùëéùëéùëñùëñ) where ùëûùëûùëñùëñ is a query from the dataset and ùëéùëéùëñùëñ is the corresponding \\nanswer. \\nThe language model ‚Ñí is Ô¨Åne-tuned on ùíüùíü, optimizing the weights to minimize the loss function, typically a \\ncross-entropy loss between the predicted and actual answers.'),\n",
       " Document(metadata={'producer': 'Adobe PDF Library 23.8.53', 'creator': 'Acrobat PDFMaker 23 for Word', 'creationdate': '2024-02-21T12:54:50-06:00', 'author': 'JOFSR', 'comments': '', 'company': '', 'keywords': '', 'moddate': '2024-02-21T12:55:38-06:00', 'sourcemodified': 'D:20240221185350', 'subject': '', 'title': '', 'source': '..\\\\data\\\\pdf\\\\A retrieval-augmented generation based large language model benchmarked on a novel dataset.pdf', 'total_pages': 9, 'page': 3, 'page_label': '4', 'source_file': 'A retrieval-augmented generation based large language model benchmarked on a novel dataset.pdf', 'file_type': 'pdf'}, page_content=\"answer. \\nThe language model ‚Ñí is Ô¨Åne-tuned on ùíüùíü, optimizing the weights to minimize the loss function, typically a \\ncross-entropy loss between the predicted and actual answers.  \\nFollowing training, the question-answering process involves feeding a new query ùëûùëû‚Ä≤ to the trained model and retrieving \\nthe answer ùëéùëé‚Ä≤. This answer is then compared to a predeÔ¨Åned list of correct answers using the similarity score, which is \\nfundamental to evaluating the model's performance. \\n \\nBenchmarking and Evaluation \\n \\nThe evaluation metric for our experiment is based on the similarity scores between the generated responses and a set \\nof reference answers. Let ùê¥ùê¥ = {ùëéùëé1\\n‚Ä≤, ùëéùëé2\\n‚Ä≤ , ‚Ä¶ , ùëéùëéùëõùëõ\\n‚Ä≤ } be the set of answers generated by the model, and ùê¥ùê¥ref = {ùëéùëé1\\n‚àó, ùëéùëé2\\n‚àó, ‚Ä¶ , ùëéùëéùëõùëõ\\n‚àó} \\nbe the set of reference answers. We deÔ¨Åne the average similarity score as follows: \\nScoreavg = 1\\nn ÔøΩsimilarity(ùêØùêØùëéùëéùëñùëñ\\n‚Ä≤, ùêØùêØùëéùëéùëñùëñ\\n‚àó)\\nùëõùëõ\\nùëñùëñ=1\"),\n",
       " Document(metadata={'producer': 'Adobe PDF Library 23.8.53', 'creator': 'Acrobat PDFMaker 23 for Word', 'creationdate': '2024-02-21T12:54:50-06:00', 'author': 'JOFSR', 'comments': '', 'company': '', 'keywords': '', 'moddate': '2024-02-21T12:55:38-06:00', 'sourcemodified': 'D:20240221185350', 'subject': '', 'title': '', 'source': '..\\\\data\\\\pdf\\\\A retrieval-augmented generation based large language model benchmarked on a novel dataset.pdf', 'total_pages': 9, 'page': 3, 'page_label': '4', 'source_file': 'A retrieval-augmented generation based large language model benchmarked on a novel dataset.pdf', 'file_type': 'pdf'}, page_content=\"‚àó, ùëéùëé2\\n‚àó, ‚Ä¶ , ùëéùëéùëõùëõ\\n‚àó} \\nbe the set of reference answers. We deÔ¨Åne the average similarity score as follows: \\nScoreavg = 1\\nn ÔøΩsimilarity(ùêØùêØùëéùëéùëñùëñ\\n‚Ä≤, ùêØùêØùëéùëéùëñùëñ\\n‚àó)\\nùëõùëõ\\nùëñùëñ=1\\n \\nThis average score acts as the primary benchmark for comparing diÔ¨Äerent model conÔ¨Ågurations. We systematically \\nrecord the scores across various combinations of language models and similarity scoring mechanisms to assess which \\nconÔ¨Ågurations yield the highest average similarity, indicating the most eÔ¨Äective model setup for our dataset. \\nAdditionally, we account for the presence or absence of context in the model's training and response genera-\\ntion. This is critical, as the presence of context has been shown to signiÔ¨Åcantly inÔ¨Çuence model performance, particu-\\nlarly in the domain of indigenous knowledge and biodiversity, where context provides essential background information \\nthat can drastically aÔ¨Äect the meaning and relevance of a response.\"),\n",
       " Document(metadata={'producer': 'Adobe PDF Library 23.8.53', 'creator': 'Acrobat PDFMaker 23 for Word', 'creationdate': '2024-02-21T12:54:50-06:00', 'author': 'JOFSR', 'comments': '', 'company': '', 'keywords': '', 'moddate': '2024-02-21T12:55:38-06:00', 'sourcemodified': 'D:20240221185350', 'subject': '', 'title': '', 'source': '..\\\\data\\\\pdf\\\\A retrieval-augmented generation based large language model benchmarked on a novel dataset.pdf', 'total_pages': 9, 'page': 3, 'page_label': '4', 'source_file': 'A retrieval-augmented generation based large language model benchmarked on a novel dataset.pdf', 'file_type': 'pdf'}, page_content=\"larly in the domain of indigenous knowledge and biodiversity, where context provides essential background information \\nthat can drastically aÔ¨Äect the meaning and relevance of a response. \\nThrough this meticulous experimental setup, we aim to illuminate the intricate dynamics between diÔ¨Äerent \\ncomponents of the RAG framework and their collective impact on the model's ability to accurately replicate and convey \\nthe richness of the Amazon Rainforest's cultural and ecological knowledge. \\n \\nPre-Experiment Performance Expectations and Discussion \\n \\nIn the landscape of varying conÔ¨Ågurations, we hypothesize that certain setups will yield higher average similarity \\nscores than others, indicative of more nuanced and accurate language generation. Particularly, we expect that: \\nThe similarity scores for models trained with contextual data ùêØùêØcontext will surpass those trained without, due to the \\nenriched understanding and background the model has of the subject matter.\"),\n",
       " Document(metadata={'producer': 'Adobe PDF Library 23.8.53', 'creator': 'Acrobat PDFMaker 23 for Word', 'creationdate': '2024-02-21T12:54:50-06:00', 'author': 'JOFSR', 'comments': '', 'company': '', 'keywords': '', 'moddate': '2024-02-21T12:55:38-06:00', 'sourcemodified': 'D:20240221185350', 'subject': '', 'title': '', 'source': '..\\\\data\\\\pdf\\\\A retrieval-augmented generation based large language model benchmarked on a novel dataset.pdf', 'total_pages': 9, 'page': 3, 'page_label': '4', 'source_file': 'A retrieval-augmented generation based large language model benchmarked on a novel dataset.pdf', 'file_type': 'pdf'}, page_content='The similarity scores for models trained with contextual data ùêØùêØcontext will surpass those trained without, due to the \\nenriched understanding and background the model has of the subject matter. \\nWhen aligning models with their native embeddings (e.g., GPT with OpenAI Embed, Palm with Palm Em-\\nbed), the semantic vector representations (ùêØùêØùë†ùë†, ùêØùêØùë°ùë°) should align more closely, thus producing higher similarity scores. \\nThe modular nature of the RAG setup will reveal that certain combinations of base language models and similarity \\nscoring mechanisms are more eÔ¨Äective than others, depending on whether context is included or not. \\nVolume 12 Issue 4 (2023) \\nISSN: 2167-1907\\nwww.JSR.org/hs\\n4'),\n",
       " Document(metadata={'producer': 'Adobe PDF Library 23.8.53', 'creator': 'Acrobat PDFMaker 23 for Word', 'creationdate': '2024-02-21T12:54:50-06:00', 'author': 'JOFSR', 'comments': '', 'company': '', 'keywords': '', 'moddate': '2024-02-21T12:55:38-06:00', 'sourcemodified': 'D:20240221185350', 'subject': '', 'title': '', 'source': '..\\\\data\\\\pdf\\\\A retrieval-augmented generation based large language model benchmarked on a novel dataset.pdf', 'total_pages': 9, 'page': 4, 'page_label': '5', 'source_file': 'A retrieval-augmented generation based large language model benchmarked on a novel dataset.pdf', 'file_type': 'pdf'}, page_content='We denote the expected performance increase due to context as  ‚àÜcontext, and the alignment of native embeddings as \\n‚àÜnative. Mathematically, we can represent our hypothesis as: \\nScoreavg,context > Scoreavg,no context + ‚àÜcontext \\nScoreavg,native > Scoreavg,non‚àínative + ‚àÜnative \\nThese hypotheses will be tested through a series of experiments, allowing us to determine the optimal model conÔ¨Ågu-\\nration for processing and generating responses reÔ¨Çective of the Amazon Rainforest dataset. \\n \\nPotential Implications for LLMs \\n \\nThe results of this experiment are expected to have signiÔ¨Åcant implications for the development and Ô¨Åne -tuning of \\nLarge Language Models (LLMs). By identifying the most eÔ¨Äective conÔ¨Ågurations, we can oÔ¨Äer insights into the adapt-\\nability of these models to specialized datasets, which is crucial for applic ations that require a high degree of cultural \\nand contextual sensitivity.'),\n",
       " Document(metadata={'producer': 'Adobe PDF Library 23.8.53', 'creator': 'Acrobat PDFMaker 23 for Word', 'creationdate': '2024-02-21T12:54:50-06:00', 'author': 'JOFSR', 'comments': '', 'company': '', 'keywords': '', 'moddate': '2024-02-21T12:55:38-06:00', 'sourcemodified': 'D:20240221185350', 'subject': '', 'title': '', 'source': '..\\\\data\\\\pdf\\\\A retrieval-augmented generation based large language model benchmarked on a novel dataset.pdf', 'total_pages': 9, 'page': 4, 'page_label': '5', 'source_file': 'A retrieval-augmented generation based large language model benchmarked on a novel dataset.pdf', 'file_type': 'pdf'}, page_content=\"ability of these models to specialized datasets, which is crucial for applic ations that require a high degree of cultural \\nand contextual sensitivity. \\nMoreover, the experiment is poised to challenge the prevailing approach to LLM training and Ô¨Åne -tuning, \\nwhich often relies on static, one-size-Ô¨Åts-all models. Our Ô¨Åndings could suggest a shift towards a more dynamic, com-\\nponent-based approach, allowing for greater Ô¨Çexibility and precision in model performance across diverse domains. \\nThe potential success of the RAG framework in this context may also pave the way for more granular im-\\nprovements in LLMs, beyond the standard metrics of accuracy and Ô¨Çuency. It may, for instance, enhance the models' \\nability to engage with and preserve less-represented languages and dialects, fostering greater inclusivity and diversity \\nin the realm of natural language processing. \\n \\nImplications for Indigenous Knowledge Preservation\"),\n",
       " Document(metadata={'producer': 'Adobe PDF Library 23.8.53', 'creator': 'Acrobat PDFMaker 23 for Word', 'creationdate': '2024-02-21T12:54:50-06:00', 'author': 'JOFSR', 'comments': '', 'company': '', 'keywords': '', 'moddate': '2024-02-21T12:55:38-06:00', 'sourcemodified': 'D:20240221185350', 'subject': '', 'title': '', 'source': '..\\\\data\\\\pdf\\\\A retrieval-augmented generation based large language model benchmarked on a novel dataset.pdf', 'total_pages': 9, 'page': 4, 'page_label': '5', 'source_file': 'A retrieval-augmented generation based large language model benchmarked on a novel dataset.pdf', 'file_type': 'pdf'}, page_content=\"in the realm of natural language processing. \\n \\nImplications for Indigenous Knowledge Preservation \\n \\nThe signiÔ¨Åcance of our experiment extends beyond the technical accomplishments within the Ô¨Åeld of natural language \\nprocessing. It serves as a testament to the power of advanced computational techniques in preserving the rich tapestry \\nof human culture, particularly the imperiled knowledge of the Amazon Rainforest's indigenous peoples. By success-\\nfully training a language model to accurately reÔ¨Çect and communicate this knowledge, we not only preserve it for future \\ngenerations but also validate the importance of linguistic and cultural diversity in our global ecosystem. \\nThis experiment, should it succeed, will demonstrate a practical application of LLMs in the service of cultural \\npreservation. It emphasizes the role that technology can play in safeguarding intangible heritage, a mission that aligns\"),\n",
       " Document(metadata={'producer': 'Adobe PDF Library 23.8.53', 'creator': 'Acrobat PDFMaker 23 for Word', 'creationdate': '2024-02-21T12:54:50-06:00', 'author': 'JOFSR', 'comments': '', 'company': '', 'keywords': '', 'moddate': '2024-02-21T12:55:38-06:00', 'sourcemodified': 'D:20240221185350', 'subject': '', 'title': '', 'source': '..\\\\data\\\\pdf\\\\A retrieval-augmented generation based large language model benchmarked on a novel dataset.pdf', 'total_pages': 9, 'page': 4, 'page_label': '5', 'source_file': 'A retrieval-augmented generation based large language model benchmarked on a novel dataset.pdf', 'file_type': 'pdf'}, page_content=\"preservation. It emphasizes the role that technology can play in safeguarding intangible heritage, a mission that aligns \\nwith the broader objectives of UNESCO's Intangible Cultural Heritage initiatives. It serves as a model for how com-\\nmunities around the world can leverage technology to protect and share their unique cultural identities and knowledge \\nsystems. \\n \\nAdvancements in RAG Framework \\n \\nFrom a methodological standpoint, our experiment is poised to contribute to the advancement of the RAG framework \\nwithin the realm of AI language models. By dissecting the RAG components and examining their interplay, we will \\ngain insights into the mechanics of modular design in language models, oÔ¨Äering a blueprint for future research and \\ndevelopment. \\nThe outcomes of this experiment could lead to the evolution of RAG into a more nuanced and adaptable \\nframework, one that can be customized for specialized datasets and applications. This adaptability is critical as the\"),\n",
       " Document(metadata={'producer': 'Adobe PDF Library 23.8.53', 'creator': 'Acrobat PDFMaker 23 for Word', 'creationdate': '2024-02-21T12:54:50-06:00', 'author': 'JOFSR', 'comments': '', 'company': '', 'keywords': '', 'moddate': '2024-02-21T12:55:38-06:00', 'sourcemodified': 'D:20240221185350', 'subject': '', 'title': '', 'source': '..\\\\data\\\\pdf\\\\A retrieval-augmented generation based large language model benchmarked on a novel dataset.pdf', 'total_pages': 9, 'page': 4, 'page_label': '5', 'source_file': 'A retrieval-augmented generation based large language model benchmarked on a novel dataset.pdf', 'file_type': 'pdf'}, page_content=\"framework, one that can be customized for specialized datasets and applications. This adaptability is critical as the \\ndemand for LLMs expands into increasingly varied and complex domains, from legal and medical to historical and \\nanthropological. \\nFurthermore, the experiment's focus on modularity could inspire a new wave of research into component -\\nbased architectures for LLMs. Such architectures may provide a more sustainable and eÔ¨Écient pathway to model im-\\nprovement, as opposed to the computationally intensive process of training large models from scratch. \\nVolume 12 Issue 4 (2023) \\nISSN: 2167-1907\\nwww.JSR.org/hs\\n5\"),\n",
       " Document(metadata={'producer': 'Adobe PDF Library 23.8.53', 'creator': 'Acrobat PDFMaker 23 for Word', 'creationdate': '2024-02-21T12:54:50-06:00', 'author': 'JOFSR', 'comments': '', 'company': '', 'keywords': '', 'moddate': '2024-02-21T12:55:38-06:00', 'sourcemodified': 'D:20240221185350', 'subject': '', 'title': '', 'source': '..\\\\data\\\\pdf\\\\A retrieval-augmented generation based large language model benchmarked on a novel dataset.pdf', 'total_pages': 9, 'page': 5, 'page_label': '6', 'source_file': 'A retrieval-augmented generation based large language model benchmarked on a novel dataset.pdf', 'file_type': 'pdf'}, page_content='In conclusion, the proposed experiment holds the potential to make signiÔ¨Åcant contributions to both the Ô¨Åeld of AI and \\nthe preservation of human cultural heritage. The insights gained could lead to a more inclusive and representative \\nfuture for LLMs, where the voices of all communities can be heard and understood. \\n \\n \\n \\n \\nFi\\ngure 1. Venn Diagram of Data Sources for RAG. This Ô¨Ågure represents a venn diagram of 3 sources of information \\n(google search results, OpenAI/Palm, proprietary data collected by the author) combined in order to create the ‚Äúout-\\nputted answer.‚Äù \\n \\n \\nFigure 2. Executive Diagram of Proposed RAG. This diagram outlines the various steps and procedures of the RAG \\nalgorithm from the input of the ‚Äúuser question‚Äù to the ‚Äúoutputted answer of the user question.‚Äù \\n \\nVolume 12 Issue 4 (2023) \\nISSN: 2167-1907\\nwww.JSR.org/hs\\n6'),\n",
       " Document(metadata={'producer': 'Adobe PDF Library 23.8.53', 'creator': 'Acrobat PDFMaker 23 for Word', 'creationdate': '2024-02-21T12:54:50-06:00', 'author': 'JOFSR', 'comments': '', 'company': '', 'keywords': '', 'moddate': '2024-02-21T12:55:38-06:00', 'sourcemodified': 'D:20240221185350', 'subject': '', 'title': '', 'source': '..\\\\data\\\\pdf\\\\A retrieval-augmented generation based large language model benchmarked on a novel dataset.pdf', 'total_pages': 9, 'page': 6, 'page_label': '7', 'source_file': 'A retrieval-augmented generation based large language model benchmarked on a novel dataset.pdf', 'file_type': 'pdf'}, page_content='Experimental Results and Discussion \\n \\nThe purpose of this section is to lay down the diÔ¨Äerent steps and customizations used within our experiment in order \\nto demonstrate the conclusive results of this experiment to the reader; our experiment using a RAG methodology \\naccurately shows how each component of a LLM positively or negatively aÔ¨Äects the accuracy of the outcome itself. \\nIn the initial world of LLM, in order to incrementally increase its performance engineers of these models would have \\nto Ô¨Åne tune them then retrain which took immense amounts of power and large amounts of null results. However, now \\nas they become more and more complex to tune models like OpenAI‚Äôs GPT and Google‚Äôs Bard have been plateauing \\nperformance wise. \\n \\nTab\\nle 1. Experimental Results. This table represents the various diÔ¨Äerent combinations of LLM components with \\nrespect to the average similarity score they each produced. \\n \\n Context LLM Embed for Similarity Score'),\n",
       " Document(metadata={'producer': 'Adobe PDF Library 23.8.53', 'creator': 'Acrobat PDFMaker 23 for Word', 'creationdate': '2024-02-21T12:54:50-06:00', 'author': 'JOFSR', 'comments': '', 'company': '', 'keywords': '', 'moddate': '2024-02-21T12:55:38-06:00', 'sourcemodified': 'D:20240221185350', 'subject': '', 'title': '', 'source': '..\\\\data\\\\pdf\\\\A retrieval-augmented generation based large language model benchmarked on a novel dataset.pdf', 'total_pages': 9, 'page': 6, 'page_label': '7', 'source_file': 'A retrieval-augmented generation based large language model benchmarked on a novel dataset.pdf', 'file_type': 'pdf'}, page_content='respect to the average similarity score they each produced. \\n \\n Context LLM Embed for Similarity Score \\n Yes No GPT Palm OpenAI Embedding Palm Embedding Score \\n1   x x   x   0.75 \\n2 x  x   x 0.92 \\n3 x   x  x 0.93 \\n4  x  x  x 0.88 \\n5 x  x  x  0.997 \\n6 x   x x  0.996 \\n7  x  x x  0.91 \\n8   x x     x 0.897 \\n \\nThis paper produces a new solution to the slowing improvement of LLM in the form of RAG, a way to com-\\nponentize the models and break them down into smaller sections. This allows the user to add certain parts / combina-\\ntions to test the performance of those then to substitute diÔ¨Äerent modules in to see which leads to the largest perfor-\\nmance increase over the other. These customizable steps allow you to see minute diÔ¨Äerences in performance that slowly \\ntuning a model couldn‚Äôt have shown you previously. This is a novel way to approach the tuning of LLM and will only \\nserve to increase their accuracy as time moves forward.'),\n",
       " Document(metadata={'producer': 'Adobe PDF Library 23.8.53', 'creator': 'Acrobat PDFMaker 23 for Word', 'creationdate': '2024-02-21T12:54:50-06:00', 'author': 'JOFSR', 'comments': '', 'company': '', 'keywords': '', 'moddate': '2024-02-21T12:55:38-06:00', 'sourcemodified': 'D:20240221185350', 'subject': '', 'title': '', 'source': '..\\\\data\\\\pdf\\\\A retrieval-augmented generation based large language model benchmarked on a novel dataset.pdf', 'total_pages': 9, 'page': 6, 'page_label': '7', 'source_file': 'A retrieval-augmented generation based large language model benchmarked on a novel dataset.pdf', 'file_type': 'pdf'}, page_content='tuning a model couldn‚Äôt have shown you previously. This is a novel way to approach the tuning of LLM and will only \\nserve to increase their accuracy as time moves forward. \\nAnother major component of our RAG methodology is the ability to switch out which embedding layer you \\nuse. The standard embedding (OpenAI) or Palm‚Äôs embed. When choosing between both of those some tradeoÔ¨Äs are \\nmade. \\nWhen using no context, Palm‚Äôs embedding layer seems to perform much better across the board, allowing for \\na much higher average similarity score, however this drastically shifts when given context as now OpenAI‚Äôs embedding \\nlayer performs much more soundly. The evidence for these claims is discussed later in this paper. \\nAdditionally, another beneÔ¨Åcial feature of the RAG optimization and breakdown style is the ability to cus-\\ntomize which similarity score the LLM uses to decide which answer to base its response oÔ¨Ä from the Q-A list. To go'),\n",
       " Document(metadata={'producer': 'Adobe PDF Library 23.8.53', 'creator': 'Acrobat PDFMaker 23 for Word', 'creationdate': '2024-02-21T12:54:50-06:00', 'author': 'JOFSR', 'comments': '', 'company': '', 'keywords': '', 'moddate': '2024-02-21T12:55:38-06:00', 'sourcemodified': 'D:20240221185350', 'subject': '', 'title': '', 'source': '..\\\\data\\\\pdf\\\\A retrieval-augmented generation based large language model benchmarked on a novel dataset.pdf', 'total_pages': 9, 'page': 6, 'page_label': '7', 'source_file': 'A retrieval-augmented generation based large language model benchmarked on a novel dataset.pdf', 'file_type': 'pdf'}, page_content='tomize which similarity score the LLM uses to decide which answer to base its response oÔ¨Ä from the Q-A list. To go \\ninto further detail, the code when prompted with a user question compares the user question to the Q-A list and reorders \\nthe list based oÔ¨Ä highest similarity score to lowest, this allows the LLM to select the top 2 -3 answers to the highest \\nranked questions and continue generating its own response from there. \\nThe Ô¨Årst choice of similarity score was STS OpenAI Score while the second was STS Palm Score. In terms \\nof the data when GPT (for the purposes of precision all of the following results include context) and STS OpenAI were \\ncombined, you got an average similarity score of 0.997. If you instead pair this with Palm STS Score instead, the \\naverage score drops to 0.92, a 0.077 decrease in performance. A similar eÔ¨Äect when using Palm with Palm STS and \\nPalm with OpenAI STS (0.996 versus 0.93, respectively). This data demonstrates that both Palm and OpenAI are able'),\n",
       " Document(metadata={'producer': 'Adobe PDF Library 23.8.53', 'creator': 'Acrobat PDFMaker 23 for Word', 'creationdate': '2024-02-21T12:54:50-06:00', 'author': 'JOFSR', 'comments': '', 'company': '', 'keywords': '', 'moddate': '2024-02-21T12:55:38-06:00', 'sourcemodified': 'D:20240221185350', 'subject': '', 'title': '', 'source': '..\\\\data\\\\pdf\\\\A retrieval-augmented generation based large language model benchmarked on a novel dataset.pdf', 'total_pages': 9, 'page': 6, 'page_label': '7', 'source_file': 'A retrieval-augmented generation based large language model benchmarked on a novel dataset.pdf', 'file_type': 'pdf'}, page_content='Palm with OpenAI STS (0.996 versus 0.93, respectively). This data demonstrates that both Palm and OpenAI are able \\nVolume 12 Issue 4 (2023) \\nISSN: 2167-1907\\nwww.JSR.org/hs\\n7'),\n",
       " Document(metadata={'producer': 'Adobe PDF Library 23.8.53', 'creator': 'Acrobat PDFMaker 23 for Word', 'creationdate': '2024-02-21T12:54:50-06:00', 'author': 'JOFSR', 'comments': '', 'company': '', 'keywords': '', 'moddate': '2024-02-21T12:55:38-06:00', 'sourcemodified': 'D:20240221185350', 'subject': '', 'title': '', 'source': '..\\\\data\\\\pdf\\\\A retrieval-augmented generation based large language model benchmarked on a novel dataset.pdf', 'total_pages': 9, 'page': 7, 'page_label': '8', 'source_file': 'A retrieval-augmented generation based large language model benchmarked on a novel dataset.pdf', 'file_type': 'pdf'}, page_content='to reach very high accuracy levels when paired with a similarity score calculated from the same program (this means \\nGPT worked better with OpenAI STS Score and that Palm worked better with Palm STS Score). What is also interest-\\ning to note is that although Palm produced a 0.001 lower performance than GPT it seemed to be more Ô¨Çexible, working \\nbetter with its competitor (OpenAI  STS Score + Palm produced 0.93) than the GPT with its competitor (Palm STS \\nScore + GPT produced 0.92). \\nTo recap on the experimental setup, this code uses an interchangeable piece of a LLM so you can swap or \\nreplace things like the embedding layer used, the similarity score used, and the base language model used, also whether \\nit was given context from the Q-A database or not. This is a novel and important way to be able to break down LLM \\nand the data collected speaks a lot to the importance of each aspect of an LLM.'),\n",
       " Document(metadata={'producer': 'Adobe PDF Library 23.8.53', 'creator': 'Acrobat PDFMaker 23 for Word', 'creationdate': '2024-02-21T12:54:50-06:00', 'author': 'JOFSR', 'comments': '', 'company': '', 'keywords': '', 'moddate': '2024-02-21T12:55:38-06:00', 'sourcemodified': 'D:20240221185350', 'subject': '', 'title': '', 'source': '..\\\\data\\\\pdf\\\\A retrieval-augmented generation based large language model benchmarked on a novel dataset.pdf', 'total_pages': 9, 'page': 7, 'page_label': '8', 'source_file': 'A retrieval-augmented generation based large language model benchmarked on a novel dataset.pdf', 'file_type': 'pdf'}, page_content='it was given context from the Q-A database or not. This is a novel and important way to be able to break down LLM \\nand the data collected speaks a lot to the importance of each aspect of an LLM. \\nIn terms of expected results, two things were noticed, Ô¨Årst that Palm had a slightly lower performance than \\nGPT (0.996 versus 0.997 respectively) on their top runs, however, there was also some contradictory data as it seemed \\nthat Palm worked signiÔ¨Åcantly better when given no context to work with compared to GPT, Palm produced an average \\nscore of 0.88 and 0.91 when given no context while GPT produced an average score of 0.75 and 0.897. Although GPT \\nmay perform much better when given context, Palm seems to beat it out just given its own proprietary dataset (no \\ncontext). \\nSome unexpected results occurred with pairing GPT and Palm with their opposite embeds, for example, pair-\\ning Palm with OpenAI embed. While on paper it makes sense that Palm would work better with Palm embed, it actually'),\n",
       " Document(metadata={'producer': 'Adobe PDF Library 23.8.53', 'creator': 'Acrobat PDFMaker 23 for Word', 'creationdate': '2024-02-21T12:54:50-06:00', 'author': 'JOFSR', 'comments': '', 'company': '', 'keywords': '', 'moddate': '2024-02-21T12:55:38-06:00', 'sourcemodified': 'D:20240221185350', 'subject': '', 'title': '', 'source': '..\\\\data\\\\pdf\\\\A retrieval-augmented generation based large language model benchmarked on a novel dataset.pdf', 'total_pages': 9, 'page': 7, 'page_label': '8', 'source_file': 'A retrieval-augmented generation based large language model benchmarked on a novel dataset.pdf', 'file_type': 'pdf'}, page_content='ing Palm with OpenAI embed. While on paper it makes sense that Palm would work better with Palm embed, it actually \\nperformed better when paired with OpenAI‚Äôs embed. 0.91 (with OpenAI embed) versus 0.88 (Palm embed)‚Äînote that \\nthis is without context given. A similar eÔ¨Äect was noticed going the other way around as well, without context, GPT  \\nperformed much better with Palm embedding layer than with its own OpenAI embedding layer (0.897 versus 0.75 \\nrespectively). This data shows how Palm embedding layer tends to perform much better given no context when com-\\npared to OpenAI ‚Äôs embed. Similarly, to explored above, it is the opposite when given context, however. OpenAI‚Äôs \\nembedding layer performs a bit better when given context across the board than Palm embed. \\nMost notably from this experiment was two realizations. First, that Palm Embedding layer tends to work much \\nbetter when just given its own proprietary dataset (and no context), when compared to OpenAI‚Äôs embed. Additionally,'),\n",
       " Document(metadata={'producer': 'Adobe PDF Library 23.8.53', 'creator': 'Acrobat PDFMaker 23 for Word', 'creationdate': '2024-02-21T12:54:50-06:00', 'author': 'JOFSR', 'comments': '', 'company': '', 'keywords': '', 'moddate': '2024-02-21T12:55:38-06:00', 'sourcemodified': 'D:20240221185350', 'subject': '', 'title': '', 'source': '..\\\\data\\\\pdf\\\\A retrieval-augmented generation based large language model benchmarked on a novel dataset.pdf', 'total_pages': 9, 'page': 7, 'page_label': '8', 'source_file': 'A retrieval-augmented generation based large language model benchmarked on a novel dataset.pdf', 'file_type': 'pdf'}, page_content='better when just given its own proprietary dataset (and no context), when compared to OpenAI‚Äôs embed. Additionally, \\nwhen given context, the playing Ô¨Åeld switches: Palm tends to perform much worse when given context when compared \\nto OpenAI. Lastly, it is important to note that a combination of Palm/GPT with OpenAI‚Äôs embedding layer and context \\nyielded extremely accurate results when its similarity scores were averaged. \\n \\nConclusion \\n \\nBuilding upon the foundation laid by our initial Ô¨Åndings, it is paramount to recognize the exceptional performance of \\nthe Palm model when utilizing its proprietary dataset in conjunction with the Palm embed. This speciÔ¨Åcity in data and \\ntechnology synchronization has shown that Palm outshines OpenAI in terms of model accuracy in a context-free envi-\\nronment. However, the landscape shifts when contextual data is integrated. In such scenarios, the combination of GPT'),\n",
       " Document(metadata={'producer': 'Adobe PDF Library 23.8.53', 'creator': 'Acrobat PDFMaker 23 for Word', 'creationdate': '2024-02-21T12:54:50-06:00', 'author': 'JOFSR', 'comments': '', 'company': '', 'keywords': '', 'moddate': '2024-02-21T12:55:38-06:00', 'sourcemodified': 'D:20240221185350', 'subject': '', 'title': '', 'source': '..\\\\data\\\\pdf\\\\A retrieval-augmented generation based large language model benchmarked on a novel dataset.pdf', 'total_pages': 9, 'page': 7, 'page_label': '8', 'source_file': 'A retrieval-augmented generation based large language model benchmarked on a novel dataset.pdf', 'file_type': 'pdf'}, page_content='ronment. However, the landscape shifts when contextual data is integrated. In such scenarios, the combination of GPT \\nwith its native OpenAI embedding layer excels, leveraging the additional context to produce responses of remarkable \\naccuracy that resonate with the cultural and ecological nuances of the Amazon. \\nThis pivot in performance based on context underscores the signiÔ¨Åcance of tailored datasets and embedding \\nmechanisms in the optimization of Large Language Models (LLMs). The adaptability of the Retrieval -Augmented \\nGeneration (RAG) framework emerges as a cornerstone for future enhancements in LLMs. By enabling the seamless \\ninterchange of model components, RAG presents an evolutionary leap in the Ô¨Åne-tuning of language models, catering \\nto the intricate demands of culturally rich and contextually complex datasets. \\nIn light of these advancements, our research signiÔ¨Åes a pivotal moment for LLMs. The evidence suggests that'),\n",
       " Document(metadata={'producer': 'Adobe PDF Library 23.8.53', 'creator': 'Acrobat PDFMaker 23 for Word', 'creationdate': '2024-02-21T12:54:50-06:00', 'author': 'JOFSR', 'comments': '', 'company': '', 'keywords': '', 'moddate': '2024-02-21T12:55:38-06:00', 'sourcemodified': 'D:20240221185350', 'subject': '', 'title': '', 'source': '..\\\\data\\\\pdf\\\\A retrieval-augmented generation based large language model benchmarked on a novel dataset.pdf', 'total_pages': 9, 'page': 7, 'page_label': '8', 'source_file': 'A retrieval-augmented generation based large language model benchmarked on a novel dataset.pdf', 'file_type': 'pdf'}, page_content=\"to the intricate demands of culturally rich and contextually complex datasets. \\nIn light of these advancements, our research signiÔ¨Åes a pivotal moment for LLMs. The evidence suggests that \\nwhen models are Ô¨Ånely tuned with an awareness of the dataset's inherent context and the corresponding embedding \\nlayers, they reach new heights of linguistic precision. Therefore, the path forward for LLMs lies in embracing the \\nmodular and contextually aware RAG framework, which promises to reÔ¨Åne the capabilities of language models to an \\nunprecedented degree, ensuring the preservation and celebration of the world's diverse linguistic and cultural heritage. \\nVolume 12 Issue 4 (2023) \\nISSN: 2167-1907\\nwww.JSR.org/hs\\n8\"),\n",
       " Document(metadata={'producer': 'Adobe PDF Library 23.8.53', 'creator': 'Acrobat PDFMaker 23 for Word', 'creationdate': '2024-02-21T12:54:50-06:00', 'author': 'JOFSR', 'comments': '', 'company': '', 'keywords': '', 'moddate': '2024-02-21T12:55:38-06:00', 'sourcemodified': 'D:20240221185350', 'subject': '', 'title': '', 'source': '..\\\\data\\\\pdf\\\\A retrieval-augmented generation based large language model benchmarked on a novel dataset.pdf', 'total_pages': 9, 'page': 8, 'page_label': '9', 'source_file': 'A retrieval-augmented generation based large language model benchmarked on a novel dataset.pdf', 'file_type': 'pdf'}, page_content='Limitations \\n \\nThe results of the ‚Äúoutputted answer‚Äù of this algorithm largely reÔ¨Çect the quality of the data. If the LLM is trained oÔ¨Ä \\nlow-quality data, then the answer will reÔ¨Çect this bias. The results of the experiment will Ô¨Çuctuate with diÔ¨Äerent results \\nshould a diÔ¨Äerent dataset be used to train the LLM.  \\nIn the future there is a lot of potential to expand on this research by breaking down the RAG algorithm into \\nev\\nen more separate components to further see the diÔ¨Äerences in average similarity score that adding or removing each \\ncomponent makes. \\n \\nRefer ences \\n \\nBahdanau, D. C. (2016). End-to-end attention-based large vocabulary speech recognition. 2016 IEEE international \\nconference on acoustics, speech and signal processing (ICASSP), 4945-4949. \\nDevlin, J. C. (2018). Bert: Pre-training of deep bidirectional transformers for language understanding. arXiv preprint, \\narXiv:1810.04805.'),\n",
       " Document(metadata={'producer': 'Adobe PDF Library 23.8.53', 'creator': 'Acrobat PDFMaker 23 for Word', 'creationdate': '2024-02-21T12:54:50-06:00', 'author': 'JOFSR', 'comments': '', 'company': '', 'keywords': '', 'moddate': '2024-02-21T12:55:38-06:00', 'sourcemodified': 'D:20240221185350', 'subject': '', 'title': '', 'source': '..\\\\data\\\\pdf\\\\A retrieval-augmented generation based large language model benchmarked on a novel dataset.pdf', 'total_pages': 9, 'page': 8, 'page_label': '9', 'source_file': 'A retrieval-augmented generation based large language model benchmarked on a novel dataset.pdf', 'file_type': 'pdf'}, page_content='Devlin, J. C. (2018). Bert: Pre-training of deep bidirectional transformers for language understanding. arXiv preprint, \\narXiv:1810.04805. \\nLewis, P. P. (2020). Retrieval-augmented generation for knowledge-intensive nlp tasks. Advances in Neural \\nInformation Processing Systems, 9459-9474. \\nRadford, A. N. (2018). Improving language understanding by generative pre-training. OpenAI. \\nSiriwardhana, S. W. (2023). Improving the domain adaptation of retrieval augmented generation (RAG) models for \\nopen domain question answering. Transactions of the Association for Computational Linguistics, 1-17. \\nVaswani, A. S. (2017). Attention is all you need. Advances in neural information processing systems. \\nYu, W. (2022). Retrieval-augmented generation across heterogeneous knowledge. Proceedings of the 2022 \\nConference of the North American Chapter of the Association for Computational Linguistics: Human Language \\nTechnologies: Student Research Workshop, 52-58. \\n \\n \\n  \\nVolume 12 Issue 4 (2023)'),\n",
       " Document(metadata={'producer': 'Adobe PDF Library 23.8.53', 'creator': 'Acrobat PDFMaker 23 for Word', 'creationdate': '2024-02-21T12:54:50-06:00', 'author': 'JOFSR', 'comments': '', 'company': '', 'keywords': '', 'moddate': '2024-02-21T12:55:38-06:00', 'sourcemodified': 'D:20240221185350', 'subject': '', 'title': '', 'source': '..\\\\data\\\\pdf\\\\A retrieval-augmented generation based large language model benchmarked on a novel dataset.pdf', 'total_pages': 9, 'page': 8, 'page_label': '9', 'source_file': 'A retrieval-augmented generation based large language model benchmarked on a novel dataset.pdf', 'file_type': 'pdf'}, page_content='Conference of the North American Chapter of the Association for Computational Linguistics: Human Language \\nTechnologies: Student Research Workshop, 52-58. \\n \\n \\n  \\nVolume 12 Issue 4 (2023) \\nISSN: 2167-1907\\nwww.JSR.org/hs\\n9')]"
      ]
     },
     "execution_count": 10,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "chunks=split_documents(all_pdf_documents)\n",
    "chunks"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "97fe92ea",
   "metadata": {},
   "source": [
    "### embedding And vectorStoreDB"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "id": "e3ae3031",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "d:\\CampusX\\RAG-using-Langchain\\.venv\\Lib\\site-packages\\tqdm\\auto.py:21: TqdmWarning: IProgress not found. Please update jupyter and ipywidgets. See https://ipywidgets.readthedocs.io/en/stable/user_install.html\n",
      "  from .autonotebook import tqdm as notebook_tqdm\n"
     ]
    }
   ],
   "source": [
    "import numpy as np\n",
    "from sentence_transformers import SentenceTransformer\n",
    "import chromadb\n",
    "from chromadb.config import Settings\n",
    "import uuid\n",
    "from typing import List, Dict, Any, Tuple\n",
    "from sklearn.metrics.pairwise import cosine_similarity"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "id": "543614c4",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Loading embedding model: all-MiniLM-L6-v2\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Xet Storage is enabled for this repo, but the 'hf_xet' package is not installed. Falling back to regular HTTP download. For better performance, install the package with: `pip install huggingface_hub[hf_xet]` or `pip install hf_xet`\n",
      "d:\\CampusX\\RAG-using-Langchain\\.venv\\Lib\\site-packages\\huggingface_hub\\file_download.py:143: UserWarning: `huggingface_hub` cache-system uses symlinks by default to efficiently store duplicated files but your machine does not support them in C:\\Users\\subhadip\\.cache\\huggingface\\hub\\models--sentence-transformers--all-MiniLM-L6-v2. Caching files will still work but in a degraded version that might require more space on your disk. This warning can be disabled by setting the `HF_HUB_DISABLE_SYMLINKS_WARNING` environment variable. For more details, see https://huggingface.co/docs/huggingface_hub/how-to-cache#limitations.\n",
      "To support symlinks on Windows, you either need to activate Developer Mode or to run Python as an administrator. In order to activate developer mode, see this article: https://docs.microsoft.com/en-us/windows/apps/get-started/enable-your-device-for-development\n",
      "  warnings.warn(message)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Model loaded successfully. Embedding dimension: 384\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "<__main__.EmbeddingManager at 0x2afc5a6de80>"
      ]
     },
     "execution_count": 12,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "class EmbeddingManager:\n",
    "    \"\"\"Handles document embedding generation using SentenceTransformer\"\"\"\n",
    "    \n",
    "    def __init__(self, model_name: str = \"all-MiniLM-L6-v2\"):\n",
    "        \"\"\"\n",
    "        Initialize the embedding manager\n",
    "        \n",
    "        Args:\n",
    "            model_name: HuggingFace model name for sentence embeddings\n",
    "        \"\"\"\n",
    "        self.model_name = model_name\n",
    "        self.model = None\n",
    "        self._load_model()\n",
    "\n",
    "    def _load_model(self):\n",
    "        \"\"\"Load the SentenceTransformer model\"\"\"\n",
    "        try:\n",
    "            print(f\"Loading embedding model: {self.model_name}\")\n",
    "            self.model = SentenceTransformer(self.model_name)\n",
    "            print(f\"Model loaded successfully. Embedding dimension: {self.model.get_sentence_embedding_dimension()}\")\n",
    "        except Exception as e:\n",
    "            print(f\"Error loading model {self.model_name}: {e}\")\n",
    "            raise\n",
    "\n",
    "    def generate_embeddings(self, texts: List[str]) -> np.ndarray:\n",
    "        \"\"\"\n",
    "        Generate embeddings for a list of texts\n",
    "        \n",
    "        Args:\n",
    "            texts: List of text strings to embed\n",
    "            \n",
    "        Returns:\n",
    "            numpy array of embeddings with shape (len(texts), embedding_dim)\n",
    "        \"\"\"\n",
    "        if not self.model:\n",
    "            raise ValueError(\"Model not loaded\")\n",
    "        \n",
    "        print(f\"Generating embeddings for {len(texts)} texts...\")\n",
    "        embeddings = self.model.encode(texts, show_progress_bar=True)\n",
    "        print(f\"Generated embeddings with shape: {embeddings.shape}\")\n",
    "        return embeddings\n",
    "\n",
    "\n",
    "## initialize the embedding manager\n",
    "\n",
    "embedding_manager=EmbeddingManager()\n",
    "embedding_manager\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "f62c9e3b",
   "metadata": {},
   "source": [
    "### VectorStore"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "id": "c276d1ca",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Vector store initialized. Collection: pdf_documents\n",
      "Existing documents in collection: 0\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "<__main__.VectorStore at 0x2afe0c65400>"
      ]
     },
     "execution_count": 13,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "class VectorStore:\n",
    "    \"\"\"Manages document embeddings in a ChromaDB vector store\"\"\"\n",
    "    \n",
    "    def __init__(self, collection_name: str = \"pdf_documents\", persist_directory: str = \"../data/vector_store\"):\n",
    "        \"\"\"\n",
    "        Initialize the vector store\n",
    "        \n",
    "        Args:\n",
    "            collection_name: Name of the ChromaDB collection\n",
    "            persist_directory: Directory to persist the vector store\n",
    "        \"\"\"\n",
    "        self.collection_name = collection_name\n",
    "        self.persist_directory = persist_directory\n",
    "        self.client = None\n",
    "        self.collection = None\n",
    "        self._initialize_store()\n",
    "\n",
    "    def _initialize_store(self):\n",
    "        \"\"\"Initialize ChromaDB client and collection\"\"\"\n",
    "        try:\n",
    "            # Create persistent ChromaDB client\n",
    "            os.makedirs(self.persist_directory, exist_ok=True)\n",
    "            self.client = chromadb.PersistentClient(path=self.persist_directory)\n",
    "            \n",
    "            # Get or create collection\n",
    "            self.collection = self.client.get_or_create_collection(\n",
    "                name=self.collection_name,\n",
    "                metadata={\"description\": \"PDF document embeddings for RAG\"}\n",
    "            )\n",
    "            print(f\"Vector store initialized. Collection: {self.collection_name}\")\n",
    "            print(f\"Existing documents in collection: {self.collection.count()}\")\n",
    "            \n",
    "        except Exception as e:\n",
    "            print(f\"Error initializing vector store: {e}\")\n",
    "            raise\n",
    "\n",
    "    def add_documents(self, documents: List[Any], embeddings: np.ndarray):\n",
    "        \"\"\"\n",
    "        Add documents and their embeddings to the vector store\n",
    "        \n",
    "        Args:\n",
    "            documents: List of LangChain documents\n",
    "            embeddings: Corresponding embeddings for the documents\n",
    "        \"\"\"\n",
    "        if len(documents) != len(embeddings):\n",
    "            raise ValueError(\"Number of documents must match number of embeddings\")\n",
    "        \n",
    "        print(f\"Adding {len(documents)} documents to vector store...\")\n",
    "        \n",
    "        # Prepare data for ChromaDB\n",
    "        ids = []\n",
    "        metadatas = []\n",
    "        documents_text = []\n",
    "        embeddings_list = []\n",
    "        \n",
    "        for i, (doc, embedding) in enumerate(zip(documents, embeddings)):\n",
    "            # Generate unique ID\n",
    "            doc_id = f\"doc_{uuid.uuid4().hex[:8]}_{i}\"\n",
    "            ids.append(doc_id)\n",
    "            \n",
    "            # Prepare metadata\n",
    "            metadata = dict(doc.metadata)\n",
    "            metadata['doc_index'] = i\n",
    "            metadata['content_length'] = len(doc.page_content)\n",
    "            metadatas.append(metadata)\n",
    "            \n",
    "            # Document content\n",
    "            documents_text.append(doc.page_content)\n",
    "            \n",
    "            # Embedding\n",
    "            embeddings_list.append(embedding.tolist())\n",
    "        \n",
    "        # Add to collection\n",
    "        try:\n",
    "            self.collection.add(\n",
    "                ids=ids,\n",
    "                embeddings=embeddings_list,\n",
    "                metadatas=metadatas,\n",
    "                documents=documents_text\n",
    "            )\n",
    "            print(f\"Successfully added {len(documents)} documents to vector store\")\n",
    "            print(f\"Total documents in collection: {self.collection.count()}\")\n",
    "            \n",
    "        except Exception as e:\n",
    "            print(f\"Error adding documents to vector store: {e}\")\n",
    "            raise\n",
    "\n",
    "vectorstore=VectorStore()\n",
    "vectorstore\n",
    "    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "id": "2d5d2c88",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[Document(metadata={'producer': 'Adobe PDF Library 23.8.53', 'creator': 'Acrobat PDFMaker 23 for Word', 'creationdate': '2024-02-21T12:54:50-06:00', 'author': 'JOFSR', 'comments': '', 'company': '', 'keywords': '', 'moddate': '2024-02-21T12:55:38-06:00', 'sourcemodified': 'D:20240221185350', 'subject': '', 'title': '', 'source': '..\\\\data\\\\pdf\\\\A retrieval-augmented generation based large language model benchmarked on a novel dataset.pdf', 'total_pages': 9, 'page': 0, 'page_label': '1', 'source_file': 'A retrieval-augmented generation based large language model benchmarked on a novel dataset.pdf', 'file_type': 'pdf'}, page_content='A Retrieval-Augmented Generation Based Large \\nLanguage Model Benchmarked on a Novel Dataset \\nKieran Pichai \\nMenlo School \\nAB\\nSTRACT \\nThe evolution of natural language processing has seen marked advancements, particularly with the advent of models \\nlike BERT, Transformers, and GPT variants, with recent additions like GPT  and Bard. This paper investigates the \\nRetrieval-Augmented Generation (RAG) framework, providing insights into its modular design and the impact of its \\nconstituent modules on performance. Leveraging a unique dataset from Amazon Rainforest natives and biologists, our \\nresearch demonstrates the signiÔ¨Åcance of preserving indigenous cultures and biodiversity. The experiment employs a \\ncustomizable RAG methodology, allowing for the interchangeability of various components, such as the base language \\nmodel and similarity score tools. Findings indicate that while GPT performs slightly better when given context, Palm'),\n",
       " Document(metadata={'producer': 'Adobe PDF Library 23.8.53', 'creator': 'Acrobat PDFMaker 23 for Word', 'creationdate': '2024-02-21T12:54:50-06:00', 'author': 'JOFSR', 'comments': '', 'company': '', 'keywords': '', 'moddate': '2024-02-21T12:55:38-06:00', 'sourcemodified': 'D:20240221185350', 'subject': '', 'title': '', 'source': '..\\\\data\\\\pdf\\\\A retrieval-augmented generation based large language model benchmarked on a novel dataset.pdf', 'total_pages': 9, 'page': 0, 'page_label': '1', 'source_file': 'A retrieval-augmented generation based large language model benchmarked on a novel dataset.pdf', 'file_type': 'pdf'}, page_content='model and similarity score tools. Findings indicate that while GPT performs slightly better when given context, Palm \\nexhibits superior performance without context. The results also suggest that models tend to perform optimally when \\npaired with similarity scores from their native platforms. Conclusively, our approach show cases the potential of a \\nmodular RAG design in optimizing language models, presenting it as a more advantageous strategy compared to tra-\\nditional Ô¨Åne-tuning of large language models. \\nIntroduction \\nThe evolution of natural language processing models has seen signiÔ¨Åcant strides from rule -based approaches in the \\nearly stages of language understanding, eventually leading to the advent of neural networks. However, the full potential \\nof these neural netw orks awaited the computational infrastructure to catch up. The pivotal moment arrived with the \\nemergence of neural machine translation (NMT), exempliÔ¨Åed by Google Translate, which marked a turning point in'),\n",
       " Document(metadata={'producer': 'Adobe PDF Library 23.8.53', 'creator': 'Acrobat PDFMaker 23 for Word', 'creationdate': '2024-02-21T12:54:50-06:00', 'author': 'JOFSR', 'comments': '', 'company': '', 'keywords': '', 'moddate': '2024-02-21T12:55:38-06:00', 'sourcemodified': 'D:20240221185350', 'subject': '', 'title': '', 'source': '..\\\\data\\\\pdf\\\\A retrieval-augmented generation based large language model benchmarked on a novel dataset.pdf', 'total_pages': 9, 'page': 0, 'page_label': '1', 'source_file': 'A retrieval-augmented generation based large language model benchmarked on a novel dataset.pdf', 'file_type': 'pdf'}, page_content='emergence of neural machine translation (NMT), exempliÔ¨Åed by Google Translate, which marked a turning point in \\nmachine language comprehension (Bahdanau, 2016). Subsequently, a plethora of advanced models, including BERT, \\nTransformers, GPT-2, and GPT-3, have emerged, driving the Ô¨Åeld forward. Recent notable additions to this landscape \\nare models like GPT and Bard (Devlin, 2018) (Vaswani, 2017) (Radford, 2018). While Ô¨Åne-tuning such models has \\nproven to be a challenging endeavor, it has become evident that Retrieval-Augmented Generation (RAG) oÔ¨Äers a prom-\\nising alternative (Lewis, 2020) (Siriwardhana, 2023) (Yu, 2022). \\nCuriously, little attention has been devoted to dissecting the individual components of RAG and their respec-\\ntive impacts on overall performance. In response to this gap, our paper undertakes a comprehensive investigation of \\nthe RAG framework and embarks on the design of RAG models from the ground up, with a focus on the modularity'),\n",
       " Document(metadata={'producer': 'Adobe PDF Library 23.8.53', 'creator': 'Acrobat PDFMaker 23 for Word', 'creationdate': '2024-02-21T12:54:50-06:00', 'author': 'JOFSR', 'comments': '', 'company': '', 'keywords': '', 'moddate': '2024-02-21T12:55:38-06:00', 'sourcemodified': 'D:20240221185350', 'subject': '', 'title': '', 'source': '..\\\\data\\\\pdf\\\\A retrieval-augmented generation based large language model benchmarked on a novel dataset.pdf', 'total_pages': 9, 'page': 0, 'page_label': '1', 'source_file': 'A retrieval-augmented generation based large language model benchmarked on a novel dataset.pdf', 'file_type': 'pdf'}, page_content='the RAG framework and embarks on the design of RAG models from the ground up, with a focus on the modularity \\nand replaceability of its constituent modules. This research seeks to contribute to a deeper understanding of the mech-\\nanisms underlying RAG and its potential for enhancing natural language understanding and generation. These Large \\nLanguage Models (LLMs) exhibit a remarkable proÔ¨Åciency in replicating human language styles, achieving a level of \\nlinguistic verisimilitude that borders on the impeccable. In light of these capabilities, it is prudent to delve into the \\nprofound signiÔ¨Åcance of the Amazon rainforest, which equates to the importance of any ethnically or racially diverse \\nnation across the globe. Within the vast expanse of the Amazon, an intricate tapestry of life unfolds, where millions \\nof distinct species intermingle. Each of these species, as rare as the other, holds a unique and intrinsic value to the'),\n",
       " Document(metadata={'producer': 'Adobe PDF Library 23.8.53', 'creator': 'Acrobat PDFMaker 23 for Word', 'creationdate': '2024-02-21T12:54:50-06:00', 'author': 'JOFSR', 'comments': '', 'company': '', 'keywords': '', 'moddate': '2024-02-21T12:55:38-06:00', 'sourcemodified': 'D:20240221185350', 'subject': '', 'title': '', 'source': '..\\\\data\\\\pdf\\\\A retrieval-augmented generation based large language model benchmarked on a novel dataset.pdf', 'total_pages': 9, 'page': 0, 'page_label': '1', 'source_file': 'A retrieval-augmented generation based large language model benchmarked on a novel dataset.pdf', 'file_type': 'pdf'}, page_content='of distinct species intermingle. Each of these species, as rare as the other, holds a unique and intrinsic value to the \\nindigenous populations who have made this ecosystem their home. The Amazon rainforest is not only a cradle of \\nbiological diversity but also a sanctuary for an array of religions and cultures, many of which teeter on the brink of \\nVolume 12 Issue 4 (2023) \\nISSN: 2167-1907\\nwww.JSR.org/hs\\n1'),\n",
       " Document(metadata={'producer': 'Adobe PDF Library 23.8.53', 'creator': 'Acrobat PDFMaker 23 for Word', 'creationdate': '2024-02-21T12:54:50-06:00', 'author': 'JOFSR', 'comments': '', 'company': '', 'keywords': '', 'moddate': '2024-02-21T12:55:38-06:00', 'sourcemodified': 'D:20240221185350', 'subject': '', 'title': '', 'source': '..\\\\data\\\\pdf\\\\A retrieval-augmented generation based large language model benchmarked on a novel dataset.pdf', 'total_pages': 9, 'page': 1, 'page_label': '2', 'source_file': 'A retrieval-augmented generation based large language model benchmarked on a novel dataset.pdf', 'file_type': 'pdf'}, page_content='oblivion. Preserving the Amazon is not merely an environmental imperative; it is an act of justice to the indigenous \\ncommunities whose ancestral lands are enshrined within its boundaries. It is a call to safeguard the memories of the \\nland, the traditions that have evolved within its embrace, and the very essence of their cultures. However, certain \\nregions of the Amazon remain shrouded in obscurity, their Ô¨Çora and fauna so rare that reliable and readily available \\ninformation is conspicuously lacking in the vast repository of knowledge available on the internet. In this context, \\nadvanced LLMs play an instrumental role in addressing this deÔ¨Åcit by facilitating the dissemination of indigenous \\nnarratives and thereby amplifying awareness and appreciation of the rich tapestry of beliefs, practices, and traditional \\nknowledge that these communities hold dear. They serve as a bridge connecting the indigenous Amazonian cultures'),\n",
       " Document(metadata={'producer': 'Adobe PDF Library 23.8.53', 'creator': 'Acrobat PDFMaker 23 for Word', 'creationdate': '2024-02-21T12:54:50-06:00', 'author': 'JOFSR', 'comments': '', 'company': '', 'keywords': '', 'moddate': '2024-02-21T12:55:38-06:00', 'sourcemodified': 'D:20240221185350', 'subject': '', 'title': '', 'source': '..\\\\data\\\\pdf\\\\A retrieval-augmented generation based large language model benchmarked on a novel dataset.pdf', 'total_pages': 9, 'page': 1, 'page_label': '2', 'source_file': 'A retrieval-augmented generation based large language model benchmarked on a novel dataset.pdf', 'file_type': 'pdf'}, page_content='knowledge that these communities hold dear. They serve as a bridge connecting the indigenous Amazonian cultures \\nwith the global community, emphasizing the paramount importance of preserving the cultural diversity interwoven \\nwithin this vast rainforest. In sum, the overarching mission of this endeavor is twofold: to document and educate the \\nWestern world about hitherto unknown cultures while concurrently ensuring the enduring preservation of these inval-\\nuable facets of human heritage and biodiversity. \\n \\nProposed Experiment \\n \\nBackground and Importance \\n \\nThe intrinsic value of indigenous knowledge, especially from regions as biodiverse and culturally rich as the Amazon \\nRainforest, cannot be overstated. This knowledge, passed down through generations, encompasses not only cultural \\nand religious beliefs but also practical insights into the local Ô¨Çora and fauna. As the modern world encroaches on these'),\n",
       " Document(metadata={'producer': 'Adobe PDF Library 23.8.53', 'creator': 'Acrobat PDFMaker 23 for Word', 'creationdate': '2024-02-21T12:54:50-06:00', 'author': 'JOFSR', 'comments': '', 'company': '', 'keywords': '', 'moddate': '2024-02-21T12:55:38-06:00', 'sourcemodified': 'D:20240221185350', 'subject': '', 'title': '', 'source': '..\\\\data\\\\pdf\\\\A retrieval-augmented generation based large language model benchmarked on a novel dataset.pdf', 'total_pages': 9, 'page': 1, 'page_label': '2', 'source_file': 'A retrieval-augmented generation based large language model benchmarked on a novel dataset.pdf', 'file_type': 'pdf'}, page_content='and religious beliefs but also practical insights into the local Ô¨Çora and fauna. As the modern world encroaches on these \\nlands, this wisdom is in peril of being lost forever. Recognizing this, our proposed experiment aims to employ a state-\\nof-the-art Retrieval- Augmented Generation (RAG) framework to capture and leverage this  vast, yet vulnerable, \\nknowledge base. \\nOur dataset, derived from interviews with Amazon Rainforest natives and biologists, is unparalleled in its \\ndepth and breadth. It includes detailed discussions on religious practices, cultural nuances, and the integral role of the \\nsurrounding ecosystem in the daily lives of these communities. This data is not just a scientiÔ¨Åc or anthropological \\nresource; it is a repository of living history and an urgent call to action for preservation eÔ¨Äorts. \\nBy integrating this unique dataset into the RAG framework, we anticipate not only the preservation of'),\n",
       " Document(metadata={'producer': 'Adobe PDF Library 23.8.53', 'creator': 'Acrobat PDFMaker 23 for Word', 'creationdate': '2024-02-21T12:54:50-06:00', 'author': 'JOFSR', 'comments': '', 'company': '', 'keywords': '', 'moddate': '2024-02-21T12:55:38-06:00', 'sourcemodified': 'D:20240221185350', 'subject': '', 'title': '', 'source': '..\\\\data\\\\pdf\\\\A retrieval-augmented generation based large language model benchmarked on a novel dataset.pdf', 'total_pages': 9, 'page': 1, 'page_label': '2', 'source_file': 'A retrieval-augmented generation based large language model benchmarked on a novel dataset.pdf', 'file_type': 'pdf'}, page_content='By integrating this unique dataset into the RAG framework, we anticipate not only the preservation of \\nknowledge but also the generation of responses that reÔ¨Çect the rich tapestry of Amazonian life. The experiment is \\ndesigned to evaluate how diÔ¨Äerent components within the RAG setup‚Äîsuch as base language models and similarity \\nscoring algorithms‚Äîcan be optimized to reÔ¨Çect the nuances captured within our dataset. In doing so, we aim to bridge \\nthe gap between advanced language models and the profound human insights found within the Amazon. \\nThe central objective of our experiment is twofold: to analyze the performance implications of modular design \\nwithin the RAG framework and to demonstrate the profound capability of such a system to preserve and communicate \\nthe wealth of indigenous knowledge. We hypothesize that a customizable RAG model will not only facilitate a deeper'),\n",
       " Document(metadata={'producer': 'Adobe PDF Library 23.8.53', 'creator': 'Acrobat PDFMaker 23 for Word', 'creationdate': '2024-02-21T12:54:50-06:00', 'author': 'JOFSR', 'comments': '', 'company': '', 'keywords': '', 'moddate': '2024-02-21T12:55:38-06:00', 'sourcemodified': 'D:20240221185350', 'subject': '', 'title': '', 'source': '..\\\\data\\\\pdf\\\\A retrieval-augmented generation based large language model benchmarked on a novel dataset.pdf', 'total_pages': 9, 'page': 1, 'page_label': '2', 'source_file': 'A retrieval-augmented generation based large language model benchmarked on a novel dataset.pdf', 'file_type': 'pdf'}, page_content='the wealth of indigenous knowledge. We hypothesize that a customizable RAG model will not only facilitate a deeper \\nunderstanding of the data but also allow us to Ô¨Åne -tune the system for optimal performance across diÔ¨Äerent conÔ¨Ågu-\\nrations. To achieve this, we will systematically explore the interchangeability of various RAG components. We will \\nassess diÔ¨Äerent base language models such as GPT and Palm and compare the eÔ¨Écacy of similarity scoring tools from \\ndiverse platforms. The experiment will rigorously test these combinations, identifying which synergies most eÔ¨Äectively \\ncapture the essence of the dataset. \\nThe ultimate goal is to showcase the potential of a modular RAG system in processing culturally signiÔ¨Åcant \\ninformation, paving the way for future applications that can beneÔ¨Åt from such tailored language models. We anticipate \\nthat our Ô¨Åndings will contribute signiÔ¨Åcantly to the Ô¨Åelds of computational linguistics and cultural preservation, demon-'),\n",
       " Document(metadata={'producer': 'Adobe PDF Library 23.8.53', 'creator': 'Acrobat PDFMaker 23 for Word', 'creationdate': '2024-02-21T12:54:50-06:00', 'author': 'JOFSR', 'comments': '', 'company': '', 'keywords': '', 'moddate': '2024-02-21T12:55:38-06:00', 'sourcemodified': 'D:20240221185350', 'subject': '', 'title': '', 'source': '..\\\\data\\\\pdf\\\\A retrieval-augmented generation based large language model benchmarked on a novel dataset.pdf', 'total_pages': 9, 'page': 1, 'page_label': '2', 'source_file': 'A retrieval-augmented generation based large language model benchmarked on a novel dataset.pdf', 'file_type': 'pdf'}, page_content='that our Ô¨Åndings will contribute signiÔ¨Åcantly to the Ô¨Åelds of computational linguistics and cultural preservation, demon-\\nstrating a novel approach to the application of large language models. \\n \\n \\n \\nVolume 12 Issue 4 (2023) \\nISSN: 2167-1907\\nwww.JSR.org/hs\\n2'),\n",
       " Document(metadata={'producer': 'Adobe PDF Library 23.8.53', 'creator': 'Acrobat PDFMaker 23 for Word', 'creationdate': '2024-02-21T12:54:50-06:00', 'author': 'JOFSR', 'comments': '', 'company': '', 'keywords': '', 'moddate': '2024-02-21T12:55:38-06:00', 'sourcemodified': 'D:20240221185350', 'subject': '', 'title': '', 'source': '..\\\\data\\\\pdf\\\\A retrieval-augmented generation based large language model benchmarked on a novel dataset.pdf', 'total_pages': 9, 'page': 2, 'page_label': '3', 'source_file': 'A retrieval-augmented generation based large language model benchmarked on a novel dataset.pdf', 'file_type': 'pdf'}, page_content='Source and Composition \\n \\nOur proprietary dataset stands as the cornerstone of this experiment. It is a rich compendium of verbal histories, inter-\\nviews, and ecological insights gathered from the indigenous peoples of the Amazon Rainforest, as well as from biolo-\\ngists and ecologists dedicated to studying this unique biome. The dataset is characterized by its diversity, comprising \\nnarratives that elucidate the intricate relationship between the natives and their environment, including the religious \\nand cultural signiÔ¨Åcance of plant and animal life. \\nThe data collection was an extensive process, where linguists and researchers engaged in deep conversations \\nwith the natives, recording their dialects, translating their stories, and documenting their knowledge of the ecological \\nsystem. Similarly, biologists contributed their decades of research on the Ô¨Çora and fauna, providing a scientiÔ¨Åc per-'),\n",
       " Document(metadata={'producer': 'Adobe PDF Library 23.8.53', 'creator': 'Acrobat PDFMaker 23 for Word', 'creationdate': '2024-02-21T12:54:50-06:00', 'author': 'JOFSR', 'comments': '', 'company': '', 'keywords': '', 'moddate': '2024-02-21T12:55:38-06:00', 'sourcemodified': 'D:20240221185350', 'subject': '', 'title': '', 'source': '..\\\\data\\\\pdf\\\\A retrieval-augmented generation based large language model benchmarked on a novel dataset.pdf', 'total_pages': 9, 'page': 2, 'page_label': '3', 'source_file': 'A retrieval-augmented generation based large language model benchmarked on a novel dataset.pdf', 'file_type': 'pdf'}, page_content=\"system. Similarly, biologists contributed their decades of research on the Ô¨Çora and fauna, providing a scientiÔ¨Åc per-\\nspective to the indigenous narratives. The data thus forms a conÔ¨Çuence of traditional wisdom and modern scientiÔ¨Åc \\nunderstanding, oÔ¨Äering a 360-degree view of the Amazon Rainforest's ecosystem. \\n \\nCultural and Environmental SigniÔ¨Åcance \\n \\nThe urgency of preserving indigenous knowledge is akin to conserving an endangered species. It is a race against time, \\nas globalization and environmental degradation threaten to erase unique cultures and the wisdom they hold. Our dataset \\nserves as a digital ark, a means to preserve and perpetuate the knowledge that has sustained the Amazon's communities \\nfor millennia. \\nThe environmental signiÔ¨Åcance of the Amazon Rainforest cannot be overstated‚Äîit is a keystone of global \\nbiodiversity. By documenting the intricate knowledge, the natives have of their environment, we are also chronicling\"),\n",
       " Document(metadata={'producer': 'Adobe PDF Library 23.8.53', 'creator': 'Acrobat PDFMaker 23 for Word', 'creationdate': '2024-02-21T12:54:50-06:00', 'author': 'JOFSR', 'comments': '', 'company': '', 'keywords': '', 'moddate': '2024-02-21T12:55:38-06:00', 'sourcemodified': 'D:20240221185350', 'subject': '', 'title': '', 'source': '..\\\\data\\\\pdf\\\\A retrieval-augmented generation based large language model benchmarked on a novel dataset.pdf', 'total_pages': 9, 'page': 2, 'page_label': '3', 'source_file': 'A retrieval-augmented generation based large language model benchmarked on a novel dataset.pdf', 'file_type': 'pdf'}, page_content=\"biodiversity. By documenting the intricate knowledge, the natives have of their environment, we are also chronicling \\nthe ecological interdependencies that are vital for the rainforest's survival. This dataset, therefore, is not just an aca-\\ndemic or technological asset; it is a critical record for environmental conservationists and policymakers. \\nThrough our experiment, we aim to amplify the voices of the Amazon's indigenous peoples, whose under-\\nstanding of their habitat is unmatched. By integrating their knowledge into the RAG framework, we hope to create a \\nmodel that not only responds with information but also with wisdom  that respects the interconnectedness of life and \\nculture. \\n \\nRetrieval-Augmented Generation Framework \\n \\nThe heart of our experiment lies in the Retrieval-Augmented Generation (RAG) framework, a sophisticated algorithm \\nthat enables the deconstruction of the language model into discrete, interchangeable components. This framework\"),\n",
       " Document(metadata={'producer': 'Adobe PDF Library 23.8.53', 'creator': 'Acrobat PDFMaker 23 for Word', 'creationdate': '2024-02-21T12:54:50-06:00', 'author': 'JOFSR', 'comments': '', 'company': '', 'keywords': '', 'moddate': '2024-02-21T12:55:38-06:00', 'sourcemodified': 'D:20240221185350', 'subject': '', 'title': '', 'source': '..\\\\data\\\\pdf\\\\A retrieval-augmented generation based large language model benchmarked on a novel dataset.pdf', 'total_pages': 9, 'page': 2, 'page_label': '3', 'source_file': 'A retrieval-augmented generation based large language model benchmarked on a novel dataset.pdf', 'file_type': 'pdf'}, page_content='that enables the deconstruction of the language model into discrete, interchangeable components. This framework \\nintegrates a retriever model t hat sources relevant context and a generator model that synthesizes the retrieved infor-\\nmation into coherent responses. \\nIn mathematical terms, given an input query ùëûùëû, the retriever model searches a knowledge base ùí¶ùí¶ and retrieves \\na set of relevant documents ùê∑ùê∑ = {ùëëùëë1, ùëëùëë2, ‚Ä¶ , ùëëùëëùëòùëò}. Each document ùëëùëë is represented as a vector ùêØùêØùëëùëë in a high-dimensional \\nspace, obtained from an embedding layer. This process transforms the raw text data into a structured form amenable \\nto computational manipulation. \\nTo examine the eÔ¨Äects of component interchangeability, we adopt various base language models and similar-\\nity scoring mechanisms. For instance, if ùê∏ùê∏denotes the embedding function, and ùë†ùë† and ùë°ùë° represent the input and target'),\n",
       " Document(metadata={'producer': 'Adobe PDF Library 23.8.53', 'creator': 'Acrobat PDFMaker 23 for Word', 'creationdate': '2024-02-21T12:54:50-06:00', 'author': 'JOFSR', 'comments': '', 'company': '', 'keywords': '', 'moddate': '2024-02-21T12:55:38-06:00', 'sourcemodified': 'D:20240221185350', 'subject': '', 'title': '', 'source': '..\\\\data\\\\pdf\\\\A retrieval-augmented generation based large language model benchmarked on a novel dataset.pdf', 'total_pages': 9, 'page': 2, 'page_label': '3', 'source_file': 'A retrieval-augmented generation based large language model benchmarked on a novel dataset.pdf', 'file_type': 'pdf'}, page_content='ity scoring mechanisms. For instance, if ùê∏ùê∏denotes the embedding function, and ùë†ùë† and ùë°ùë° represent the input and target \\ntext sequences, respectively, their vector representations would be ùêØùêØùë†ùë† = ùê∏ùê∏(ùë†ùë†) and ùêØùêØùë°ùë° = ùê∏ùê∏(ùë°ùë°). We employ cosine simi-\\nlarity as the basis for our similarity score, deÔ¨Åned by the formula: \\nsimilarity(ùêØùêØùë†ùë†, ùêØùêØùë°ùë°) = ùêØùêØùë†ùë† ‚ãÖùêØùêØùë°ùë°\\nÔøΩ|vùë†ùë†|ÔøΩ ÔøΩ|vùë°ùë°|ÔøΩ \\nHere, ‚ãÖ denotes the dot product between the two vectors, and ÔøΩ|‚ãÖ|ÔøΩ denotes the Euclidean norm. This score quantiÔ¨Åes \\nthe closeness of the semantic meaning represented by the vectors, with a value of 1 indicating identical directionality \\nand thus, maximal similarity.  \\nVolume 12 Issue 4 (2023) \\nISSN: 2167-1907\\nwww.JSR.org/hs\\n3'),\n",
       " Document(metadata={'producer': 'Adobe PDF Library 23.8.53', 'creator': 'Acrobat PDFMaker 23 for Word', 'creationdate': '2024-02-21T12:54:50-06:00', 'author': 'JOFSR', 'comments': '', 'company': '', 'keywords': '', 'moddate': '2024-02-21T12:55:38-06:00', 'sourcemodified': 'D:20240221185350', 'subject': '', 'title': '', 'source': '..\\\\data\\\\pdf\\\\A retrieval-augmented generation based large language model benchmarked on a novel dataset.pdf', 'total_pages': 9, 'page': 3, 'page_label': '4', 'source_file': 'A retrieval-augmented generation based large language model benchmarked on a novel dataset.pdf', 'file_type': 'pdf'}, page_content='The experiment tests diÔ¨Äerent conÔ¨Ågurations by substituting ùê∏ùê∏ with embedding functions from various mod-\\nels (e.g., GPT, Palm), allowing us to discern the impact of the embedding layer on the Ô¨Ånal similarity score. By com-\\nparing the performance of diÔ¨Äerent  ùê∏ùê∏ choices, we can identify which embeddings yield the most semantically rich \\nrepresentations for our unique dataset. \\n \\nExperiment Setup \\n \\nThe experiment commences with the training of the language models using our unique dataset. For the training phase, \\nwe deÔ¨Åne the following: \\n‚Ñí: The base language model, which can be either GPT or Palm. \\nùíüùíü: The training dataset, consisting of pairs  (ùëûùëûùëñùëñ, ùëéùëéùëñùëñ) where ùëûùëûùëñùëñ is a query from the dataset and ùëéùëéùëñùëñ is the corresponding \\nanswer. \\nThe language model ‚Ñí is Ô¨Åne-tuned on ùíüùíü, optimizing the weights to minimize the loss function, typically a \\ncross-entropy loss between the predicted and actual answers.'),\n",
       " Document(metadata={'producer': 'Adobe PDF Library 23.8.53', 'creator': 'Acrobat PDFMaker 23 for Word', 'creationdate': '2024-02-21T12:54:50-06:00', 'author': 'JOFSR', 'comments': '', 'company': '', 'keywords': '', 'moddate': '2024-02-21T12:55:38-06:00', 'sourcemodified': 'D:20240221185350', 'subject': '', 'title': '', 'source': '..\\\\data\\\\pdf\\\\A retrieval-augmented generation based large language model benchmarked on a novel dataset.pdf', 'total_pages': 9, 'page': 3, 'page_label': '4', 'source_file': 'A retrieval-augmented generation based large language model benchmarked on a novel dataset.pdf', 'file_type': 'pdf'}, page_content=\"answer. \\nThe language model ‚Ñí is Ô¨Åne-tuned on ùíüùíü, optimizing the weights to minimize the loss function, typically a \\ncross-entropy loss between the predicted and actual answers.  \\nFollowing training, the question-answering process involves feeding a new query ùëûùëû‚Ä≤ to the trained model and retrieving \\nthe answer ùëéùëé‚Ä≤. This answer is then compared to a predeÔ¨Åned list of correct answers using the similarity score, which is \\nfundamental to evaluating the model's performance. \\n \\nBenchmarking and Evaluation \\n \\nThe evaluation metric for our experiment is based on the similarity scores between the generated responses and a set \\nof reference answers. Let ùê¥ùê¥ = {ùëéùëé1\\n‚Ä≤, ùëéùëé2\\n‚Ä≤ , ‚Ä¶ , ùëéùëéùëõùëõ\\n‚Ä≤ } be the set of answers generated by the model, and ùê¥ùê¥ref = {ùëéùëé1\\n‚àó, ùëéùëé2\\n‚àó, ‚Ä¶ , ùëéùëéùëõùëõ\\n‚àó} \\nbe the set of reference answers. We deÔ¨Åne the average similarity score as follows: \\nScoreavg = 1\\nn ÔøΩsimilarity(ùêØùêØùëéùëéùëñùëñ\\n‚Ä≤, ùêØùêØùëéùëéùëñùëñ\\n‚àó)\\nùëõùëõ\\nùëñùëñ=1\"),\n",
       " Document(metadata={'producer': 'Adobe PDF Library 23.8.53', 'creator': 'Acrobat PDFMaker 23 for Word', 'creationdate': '2024-02-21T12:54:50-06:00', 'author': 'JOFSR', 'comments': '', 'company': '', 'keywords': '', 'moddate': '2024-02-21T12:55:38-06:00', 'sourcemodified': 'D:20240221185350', 'subject': '', 'title': '', 'source': '..\\\\data\\\\pdf\\\\A retrieval-augmented generation based large language model benchmarked on a novel dataset.pdf', 'total_pages': 9, 'page': 3, 'page_label': '4', 'source_file': 'A retrieval-augmented generation based large language model benchmarked on a novel dataset.pdf', 'file_type': 'pdf'}, page_content=\"‚àó, ùëéùëé2\\n‚àó, ‚Ä¶ , ùëéùëéùëõùëõ\\n‚àó} \\nbe the set of reference answers. We deÔ¨Åne the average similarity score as follows: \\nScoreavg = 1\\nn ÔøΩsimilarity(ùêØùêØùëéùëéùëñùëñ\\n‚Ä≤, ùêØùêØùëéùëéùëñùëñ\\n‚àó)\\nùëõùëõ\\nùëñùëñ=1\\n \\nThis average score acts as the primary benchmark for comparing diÔ¨Äerent model conÔ¨Ågurations. We systematically \\nrecord the scores across various combinations of language models and similarity scoring mechanisms to assess which \\nconÔ¨Ågurations yield the highest average similarity, indicating the most eÔ¨Äective model setup for our dataset. \\nAdditionally, we account for the presence or absence of context in the model's training and response genera-\\ntion. This is critical, as the presence of context has been shown to signiÔ¨Åcantly inÔ¨Çuence model performance, particu-\\nlarly in the domain of indigenous knowledge and biodiversity, where context provides essential background information \\nthat can drastically aÔ¨Äect the meaning and relevance of a response.\"),\n",
       " Document(metadata={'producer': 'Adobe PDF Library 23.8.53', 'creator': 'Acrobat PDFMaker 23 for Word', 'creationdate': '2024-02-21T12:54:50-06:00', 'author': 'JOFSR', 'comments': '', 'company': '', 'keywords': '', 'moddate': '2024-02-21T12:55:38-06:00', 'sourcemodified': 'D:20240221185350', 'subject': '', 'title': '', 'source': '..\\\\data\\\\pdf\\\\A retrieval-augmented generation based large language model benchmarked on a novel dataset.pdf', 'total_pages': 9, 'page': 3, 'page_label': '4', 'source_file': 'A retrieval-augmented generation based large language model benchmarked on a novel dataset.pdf', 'file_type': 'pdf'}, page_content=\"larly in the domain of indigenous knowledge and biodiversity, where context provides essential background information \\nthat can drastically aÔ¨Äect the meaning and relevance of a response. \\nThrough this meticulous experimental setup, we aim to illuminate the intricate dynamics between diÔ¨Äerent \\ncomponents of the RAG framework and their collective impact on the model's ability to accurately replicate and convey \\nthe richness of the Amazon Rainforest's cultural and ecological knowledge. \\n \\nPre-Experiment Performance Expectations and Discussion \\n \\nIn the landscape of varying conÔ¨Ågurations, we hypothesize that certain setups will yield higher average similarity \\nscores than others, indicative of more nuanced and accurate language generation. Particularly, we expect that: \\nThe similarity scores for models trained with contextual data ùêØùêØcontext will surpass those trained without, due to the \\nenriched understanding and background the model has of the subject matter.\"),\n",
       " Document(metadata={'producer': 'Adobe PDF Library 23.8.53', 'creator': 'Acrobat PDFMaker 23 for Word', 'creationdate': '2024-02-21T12:54:50-06:00', 'author': 'JOFSR', 'comments': '', 'company': '', 'keywords': '', 'moddate': '2024-02-21T12:55:38-06:00', 'sourcemodified': 'D:20240221185350', 'subject': '', 'title': '', 'source': '..\\\\data\\\\pdf\\\\A retrieval-augmented generation based large language model benchmarked on a novel dataset.pdf', 'total_pages': 9, 'page': 3, 'page_label': '4', 'source_file': 'A retrieval-augmented generation based large language model benchmarked on a novel dataset.pdf', 'file_type': 'pdf'}, page_content='The similarity scores for models trained with contextual data ùêØùêØcontext will surpass those trained without, due to the \\nenriched understanding and background the model has of the subject matter. \\nWhen aligning models with their native embeddings (e.g., GPT with OpenAI Embed, Palm with Palm Em-\\nbed), the semantic vector representations (ùêØùêØùë†ùë†, ùêØùêØùë°ùë°) should align more closely, thus producing higher similarity scores. \\nThe modular nature of the RAG setup will reveal that certain combinations of base language models and similarity \\nscoring mechanisms are more eÔ¨Äective than others, depending on whether context is included or not. \\nVolume 12 Issue 4 (2023) \\nISSN: 2167-1907\\nwww.JSR.org/hs\\n4'),\n",
       " Document(metadata={'producer': 'Adobe PDF Library 23.8.53', 'creator': 'Acrobat PDFMaker 23 for Word', 'creationdate': '2024-02-21T12:54:50-06:00', 'author': 'JOFSR', 'comments': '', 'company': '', 'keywords': '', 'moddate': '2024-02-21T12:55:38-06:00', 'sourcemodified': 'D:20240221185350', 'subject': '', 'title': '', 'source': '..\\\\data\\\\pdf\\\\A retrieval-augmented generation based large language model benchmarked on a novel dataset.pdf', 'total_pages': 9, 'page': 4, 'page_label': '5', 'source_file': 'A retrieval-augmented generation based large language model benchmarked on a novel dataset.pdf', 'file_type': 'pdf'}, page_content='We denote the expected performance increase due to context as  ‚àÜcontext, and the alignment of native embeddings as \\n‚àÜnative. Mathematically, we can represent our hypothesis as: \\nScoreavg,context > Scoreavg,no context + ‚àÜcontext \\nScoreavg,native > Scoreavg,non‚àínative + ‚àÜnative \\nThese hypotheses will be tested through a series of experiments, allowing us to determine the optimal model conÔ¨Ågu-\\nration for processing and generating responses reÔ¨Çective of the Amazon Rainforest dataset. \\n \\nPotential Implications for LLMs \\n \\nThe results of this experiment are expected to have signiÔ¨Åcant implications for the development and Ô¨Åne -tuning of \\nLarge Language Models (LLMs). By identifying the most eÔ¨Äective conÔ¨Ågurations, we can oÔ¨Äer insights into the adapt-\\nability of these models to specialized datasets, which is crucial for applic ations that require a high degree of cultural \\nand contextual sensitivity.'),\n",
       " Document(metadata={'producer': 'Adobe PDF Library 23.8.53', 'creator': 'Acrobat PDFMaker 23 for Word', 'creationdate': '2024-02-21T12:54:50-06:00', 'author': 'JOFSR', 'comments': '', 'company': '', 'keywords': '', 'moddate': '2024-02-21T12:55:38-06:00', 'sourcemodified': 'D:20240221185350', 'subject': '', 'title': '', 'source': '..\\\\data\\\\pdf\\\\A retrieval-augmented generation based large language model benchmarked on a novel dataset.pdf', 'total_pages': 9, 'page': 4, 'page_label': '5', 'source_file': 'A retrieval-augmented generation based large language model benchmarked on a novel dataset.pdf', 'file_type': 'pdf'}, page_content=\"ability of these models to specialized datasets, which is crucial for applic ations that require a high degree of cultural \\nand contextual sensitivity. \\nMoreover, the experiment is poised to challenge the prevailing approach to LLM training and Ô¨Åne -tuning, \\nwhich often relies on static, one-size-Ô¨Åts-all models. Our Ô¨Åndings could suggest a shift towards a more dynamic, com-\\nponent-based approach, allowing for greater Ô¨Çexibility and precision in model performance across diverse domains. \\nThe potential success of the RAG framework in this context may also pave the way for more granular im-\\nprovements in LLMs, beyond the standard metrics of accuracy and Ô¨Çuency. It may, for instance, enhance the models' \\nability to engage with and preserve less-represented languages and dialects, fostering greater inclusivity and diversity \\nin the realm of natural language processing. \\n \\nImplications for Indigenous Knowledge Preservation\"),\n",
       " Document(metadata={'producer': 'Adobe PDF Library 23.8.53', 'creator': 'Acrobat PDFMaker 23 for Word', 'creationdate': '2024-02-21T12:54:50-06:00', 'author': 'JOFSR', 'comments': '', 'company': '', 'keywords': '', 'moddate': '2024-02-21T12:55:38-06:00', 'sourcemodified': 'D:20240221185350', 'subject': '', 'title': '', 'source': '..\\\\data\\\\pdf\\\\A retrieval-augmented generation based large language model benchmarked on a novel dataset.pdf', 'total_pages': 9, 'page': 4, 'page_label': '5', 'source_file': 'A retrieval-augmented generation based large language model benchmarked on a novel dataset.pdf', 'file_type': 'pdf'}, page_content=\"in the realm of natural language processing. \\n \\nImplications for Indigenous Knowledge Preservation \\n \\nThe signiÔ¨Åcance of our experiment extends beyond the technical accomplishments within the Ô¨Åeld of natural language \\nprocessing. It serves as a testament to the power of advanced computational techniques in preserving the rich tapestry \\nof human culture, particularly the imperiled knowledge of the Amazon Rainforest's indigenous peoples. By success-\\nfully training a language model to accurately reÔ¨Çect and communicate this knowledge, we not only preserve it for future \\ngenerations but also validate the importance of linguistic and cultural diversity in our global ecosystem. \\nThis experiment, should it succeed, will demonstrate a practical application of LLMs in the service of cultural \\npreservation. It emphasizes the role that technology can play in safeguarding intangible heritage, a mission that aligns\"),\n",
       " Document(metadata={'producer': 'Adobe PDF Library 23.8.53', 'creator': 'Acrobat PDFMaker 23 for Word', 'creationdate': '2024-02-21T12:54:50-06:00', 'author': 'JOFSR', 'comments': '', 'company': '', 'keywords': '', 'moddate': '2024-02-21T12:55:38-06:00', 'sourcemodified': 'D:20240221185350', 'subject': '', 'title': '', 'source': '..\\\\data\\\\pdf\\\\A retrieval-augmented generation based large language model benchmarked on a novel dataset.pdf', 'total_pages': 9, 'page': 4, 'page_label': '5', 'source_file': 'A retrieval-augmented generation based large language model benchmarked on a novel dataset.pdf', 'file_type': 'pdf'}, page_content=\"preservation. It emphasizes the role that technology can play in safeguarding intangible heritage, a mission that aligns \\nwith the broader objectives of UNESCO's Intangible Cultural Heritage initiatives. It serves as a model for how com-\\nmunities around the world can leverage technology to protect and share their unique cultural identities and knowledge \\nsystems. \\n \\nAdvancements in RAG Framework \\n \\nFrom a methodological standpoint, our experiment is poised to contribute to the advancement of the RAG framework \\nwithin the realm of AI language models. By dissecting the RAG components and examining their interplay, we will \\ngain insights into the mechanics of modular design in language models, oÔ¨Äering a blueprint for future research and \\ndevelopment. \\nThe outcomes of this experiment could lead to the evolution of RAG into a more nuanced and adaptable \\nframework, one that can be customized for specialized datasets and applications. This adaptability is critical as the\"),\n",
       " Document(metadata={'producer': 'Adobe PDF Library 23.8.53', 'creator': 'Acrobat PDFMaker 23 for Word', 'creationdate': '2024-02-21T12:54:50-06:00', 'author': 'JOFSR', 'comments': '', 'company': '', 'keywords': '', 'moddate': '2024-02-21T12:55:38-06:00', 'sourcemodified': 'D:20240221185350', 'subject': '', 'title': '', 'source': '..\\\\data\\\\pdf\\\\A retrieval-augmented generation based large language model benchmarked on a novel dataset.pdf', 'total_pages': 9, 'page': 4, 'page_label': '5', 'source_file': 'A retrieval-augmented generation based large language model benchmarked on a novel dataset.pdf', 'file_type': 'pdf'}, page_content=\"framework, one that can be customized for specialized datasets and applications. This adaptability is critical as the \\ndemand for LLMs expands into increasingly varied and complex domains, from legal and medical to historical and \\nanthropological. \\nFurthermore, the experiment's focus on modularity could inspire a new wave of research into component -\\nbased architectures for LLMs. Such architectures may provide a more sustainable and eÔ¨Écient pathway to model im-\\nprovement, as opposed to the computationally intensive process of training large models from scratch. \\nVolume 12 Issue 4 (2023) \\nISSN: 2167-1907\\nwww.JSR.org/hs\\n5\"),\n",
       " Document(metadata={'producer': 'Adobe PDF Library 23.8.53', 'creator': 'Acrobat PDFMaker 23 for Word', 'creationdate': '2024-02-21T12:54:50-06:00', 'author': 'JOFSR', 'comments': '', 'company': '', 'keywords': '', 'moddate': '2024-02-21T12:55:38-06:00', 'sourcemodified': 'D:20240221185350', 'subject': '', 'title': '', 'source': '..\\\\data\\\\pdf\\\\A retrieval-augmented generation based large language model benchmarked on a novel dataset.pdf', 'total_pages': 9, 'page': 5, 'page_label': '6', 'source_file': 'A retrieval-augmented generation based large language model benchmarked on a novel dataset.pdf', 'file_type': 'pdf'}, page_content='In conclusion, the proposed experiment holds the potential to make signiÔ¨Åcant contributions to both the Ô¨Åeld of AI and \\nthe preservation of human cultural heritage. The insights gained could lead to a more inclusive and representative \\nfuture for LLMs, where the voices of all communities can be heard and understood. \\n \\n \\n \\n \\nFi\\ngure 1. Venn Diagram of Data Sources for RAG. This Ô¨Ågure represents a venn diagram of 3 sources of information \\n(google search results, OpenAI/Palm, proprietary data collected by the author) combined in order to create the ‚Äúout-\\nputted answer.‚Äù \\n \\n \\nFigure 2. Executive Diagram of Proposed RAG. This diagram outlines the various steps and procedures of the RAG \\nalgorithm from the input of the ‚Äúuser question‚Äù to the ‚Äúoutputted answer of the user question.‚Äù \\n \\nVolume 12 Issue 4 (2023) \\nISSN: 2167-1907\\nwww.JSR.org/hs\\n6'),\n",
       " Document(metadata={'producer': 'Adobe PDF Library 23.8.53', 'creator': 'Acrobat PDFMaker 23 for Word', 'creationdate': '2024-02-21T12:54:50-06:00', 'author': 'JOFSR', 'comments': '', 'company': '', 'keywords': '', 'moddate': '2024-02-21T12:55:38-06:00', 'sourcemodified': 'D:20240221185350', 'subject': '', 'title': '', 'source': '..\\\\data\\\\pdf\\\\A retrieval-augmented generation based large language model benchmarked on a novel dataset.pdf', 'total_pages': 9, 'page': 6, 'page_label': '7', 'source_file': 'A retrieval-augmented generation based large language model benchmarked on a novel dataset.pdf', 'file_type': 'pdf'}, page_content='Experimental Results and Discussion \\n \\nThe purpose of this section is to lay down the diÔ¨Äerent steps and customizations used within our experiment in order \\nto demonstrate the conclusive results of this experiment to the reader; our experiment using a RAG methodology \\naccurately shows how each component of a LLM positively or negatively aÔ¨Äects the accuracy of the outcome itself. \\nIn the initial world of LLM, in order to incrementally increase its performance engineers of these models would have \\nto Ô¨Åne tune them then retrain which took immense amounts of power and large amounts of null results. However, now \\nas they become more and more complex to tune models like OpenAI‚Äôs GPT and Google‚Äôs Bard have been plateauing \\nperformance wise. \\n \\nTab\\nle 1. Experimental Results. This table represents the various diÔ¨Äerent combinations of LLM components with \\nrespect to the average similarity score they each produced. \\n \\n Context LLM Embed for Similarity Score'),\n",
       " Document(metadata={'producer': 'Adobe PDF Library 23.8.53', 'creator': 'Acrobat PDFMaker 23 for Word', 'creationdate': '2024-02-21T12:54:50-06:00', 'author': 'JOFSR', 'comments': '', 'company': '', 'keywords': '', 'moddate': '2024-02-21T12:55:38-06:00', 'sourcemodified': 'D:20240221185350', 'subject': '', 'title': '', 'source': '..\\\\data\\\\pdf\\\\A retrieval-augmented generation based large language model benchmarked on a novel dataset.pdf', 'total_pages': 9, 'page': 6, 'page_label': '7', 'source_file': 'A retrieval-augmented generation based large language model benchmarked on a novel dataset.pdf', 'file_type': 'pdf'}, page_content='respect to the average similarity score they each produced. \\n \\n Context LLM Embed for Similarity Score \\n Yes No GPT Palm OpenAI Embedding Palm Embedding Score \\n1   x x   x   0.75 \\n2 x  x   x 0.92 \\n3 x   x  x 0.93 \\n4  x  x  x 0.88 \\n5 x  x  x  0.997 \\n6 x   x x  0.996 \\n7  x  x x  0.91 \\n8   x x     x 0.897 \\n \\nThis paper produces a new solution to the slowing improvement of LLM in the form of RAG, a way to com-\\nponentize the models and break them down into smaller sections. This allows the user to add certain parts / combina-\\ntions to test the performance of those then to substitute diÔ¨Äerent modules in to see which leads to the largest perfor-\\nmance increase over the other. These customizable steps allow you to see minute diÔ¨Äerences in performance that slowly \\ntuning a model couldn‚Äôt have shown you previously. This is a novel way to approach the tuning of LLM and will only \\nserve to increase their accuracy as time moves forward.'),\n",
       " Document(metadata={'producer': 'Adobe PDF Library 23.8.53', 'creator': 'Acrobat PDFMaker 23 for Word', 'creationdate': '2024-02-21T12:54:50-06:00', 'author': 'JOFSR', 'comments': '', 'company': '', 'keywords': '', 'moddate': '2024-02-21T12:55:38-06:00', 'sourcemodified': 'D:20240221185350', 'subject': '', 'title': '', 'source': '..\\\\data\\\\pdf\\\\A retrieval-augmented generation based large language model benchmarked on a novel dataset.pdf', 'total_pages': 9, 'page': 6, 'page_label': '7', 'source_file': 'A retrieval-augmented generation based large language model benchmarked on a novel dataset.pdf', 'file_type': 'pdf'}, page_content='tuning a model couldn‚Äôt have shown you previously. This is a novel way to approach the tuning of LLM and will only \\nserve to increase their accuracy as time moves forward. \\nAnother major component of our RAG methodology is the ability to switch out which embedding layer you \\nuse. The standard embedding (OpenAI) or Palm‚Äôs embed. When choosing between both of those some tradeoÔ¨Äs are \\nmade. \\nWhen using no context, Palm‚Äôs embedding layer seems to perform much better across the board, allowing for \\na much higher average similarity score, however this drastically shifts when given context as now OpenAI‚Äôs embedding \\nlayer performs much more soundly. The evidence for these claims is discussed later in this paper. \\nAdditionally, another beneÔ¨Åcial feature of the RAG optimization and breakdown style is the ability to cus-\\ntomize which similarity score the LLM uses to decide which answer to base its response oÔ¨Ä from the Q-A list. To go'),\n",
       " Document(metadata={'producer': 'Adobe PDF Library 23.8.53', 'creator': 'Acrobat PDFMaker 23 for Word', 'creationdate': '2024-02-21T12:54:50-06:00', 'author': 'JOFSR', 'comments': '', 'company': '', 'keywords': '', 'moddate': '2024-02-21T12:55:38-06:00', 'sourcemodified': 'D:20240221185350', 'subject': '', 'title': '', 'source': '..\\\\data\\\\pdf\\\\A retrieval-augmented generation based large language model benchmarked on a novel dataset.pdf', 'total_pages': 9, 'page': 6, 'page_label': '7', 'source_file': 'A retrieval-augmented generation based large language model benchmarked on a novel dataset.pdf', 'file_type': 'pdf'}, page_content='tomize which similarity score the LLM uses to decide which answer to base its response oÔ¨Ä from the Q-A list. To go \\ninto further detail, the code when prompted with a user question compares the user question to the Q-A list and reorders \\nthe list based oÔ¨Ä highest similarity score to lowest, this allows the LLM to select the top 2 -3 answers to the highest \\nranked questions and continue generating its own response from there. \\nThe Ô¨Årst choice of similarity score was STS OpenAI Score while the second was STS Palm Score. In terms \\nof the data when GPT (for the purposes of precision all of the following results include context) and STS OpenAI were \\ncombined, you got an average similarity score of 0.997. If you instead pair this with Palm STS Score instead, the \\naverage score drops to 0.92, a 0.077 decrease in performance. A similar eÔ¨Äect when using Palm with Palm STS and \\nPalm with OpenAI STS (0.996 versus 0.93, respectively). This data demonstrates that both Palm and OpenAI are able'),\n",
       " Document(metadata={'producer': 'Adobe PDF Library 23.8.53', 'creator': 'Acrobat PDFMaker 23 for Word', 'creationdate': '2024-02-21T12:54:50-06:00', 'author': 'JOFSR', 'comments': '', 'company': '', 'keywords': '', 'moddate': '2024-02-21T12:55:38-06:00', 'sourcemodified': 'D:20240221185350', 'subject': '', 'title': '', 'source': '..\\\\data\\\\pdf\\\\A retrieval-augmented generation based large language model benchmarked on a novel dataset.pdf', 'total_pages': 9, 'page': 6, 'page_label': '7', 'source_file': 'A retrieval-augmented generation based large language model benchmarked on a novel dataset.pdf', 'file_type': 'pdf'}, page_content='Palm with OpenAI STS (0.996 versus 0.93, respectively). This data demonstrates that both Palm and OpenAI are able \\nVolume 12 Issue 4 (2023) \\nISSN: 2167-1907\\nwww.JSR.org/hs\\n7'),\n",
       " Document(metadata={'producer': 'Adobe PDF Library 23.8.53', 'creator': 'Acrobat PDFMaker 23 for Word', 'creationdate': '2024-02-21T12:54:50-06:00', 'author': 'JOFSR', 'comments': '', 'company': '', 'keywords': '', 'moddate': '2024-02-21T12:55:38-06:00', 'sourcemodified': 'D:20240221185350', 'subject': '', 'title': '', 'source': '..\\\\data\\\\pdf\\\\A retrieval-augmented generation based large language model benchmarked on a novel dataset.pdf', 'total_pages': 9, 'page': 7, 'page_label': '8', 'source_file': 'A retrieval-augmented generation based large language model benchmarked on a novel dataset.pdf', 'file_type': 'pdf'}, page_content='to reach very high accuracy levels when paired with a similarity score calculated from the same program (this means \\nGPT worked better with OpenAI STS Score and that Palm worked better with Palm STS Score). What is also interest-\\ning to note is that although Palm produced a 0.001 lower performance than GPT it seemed to be more Ô¨Çexible, working \\nbetter with its competitor (OpenAI  STS Score + Palm produced 0.93) than the GPT with its competitor (Palm STS \\nScore + GPT produced 0.92). \\nTo recap on the experimental setup, this code uses an interchangeable piece of a LLM so you can swap or \\nreplace things like the embedding layer used, the similarity score used, and the base language model used, also whether \\nit was given context from the Q-A database or not. This is a novel and important way to be able to break down LLM \\nand the data collected speaks a lot to the importance of each aspect of an LLM.'),\n",
       " Document(metadata={'producer': 'Adobe PDF Library 23.8.53', 'creator': 'Acrobat PDFMaker 23 for Word', 'creationdate': '2024-02-21T12:54:50-06:00', 'author': 'JOFSR', 'comments': '', 'company': '', 'keywords': '', 'moddate': '2024-02-21T12:55:38-06:00', 'sourcemodified': 'D:20240221185350', 'subject': '', 'title': '', 'source': '..\\\\data\\\\pdf\\\\A retrieval-augmented generation based large language model benchmarked on a novel dataset.pdf', 'total_pages': 9, 'page': 7, 'page_label': '8', 'source_file': 'A retrieval-augmented generation based large language model benchmarked on a novel dataset.pdf', 'file_type': 'pdf'}, page_content='it was given context from the Q-A database or not. This is a novel and important way to be able to break down LLM \\nand the data collected speaks a lot to the importance of each aspect of an LLM. \\nIn terms of expected results, two things were noticed, Ô¨Årst that Palm had a slightly lower performance than \\nGPT (0.996 versus 0.997 respectively) on their top runs, however, there was also some contradictory data as it seemed \\nthat Palm worked signiÔ¨Åcantly better when given no context to work with compared to GPT, Palm produced an average \\nscore of 0.88 and 0.91 when given no context while GPT produced an average score of 0.75 and 0.897. Although GPT \\nmay perform much better when given context, Palm seems to beat it out just given its own proprietary dataset (no \\ncontext). \\nSome unexpected results occurred with pairing GPT and Palm with their opposite embeds, for example, pair-\\ning Palm with OpenAI embed. While on paper it makes sense that Palm would work better with Palm embed, it actually'),\n",
       " Document(metadata={'producer': 'Adobe PDF Library 23.8.53', 'creator': 'Acrobat PDFMaker 23 for Word', 'creationdate': '2024-02-21T12:54:50-06:00', 'author': 'JOFSR', 'comments': '', 'company': '', 'keywords': '', 'moddate': '2024-02-21T12:55:38-06:00', 'sourcemodified': 'D:20240221185350', 'subject': '', 'title': '', 'source': '..\\\\data\\\\pdf\\\\A retrieval-augmented generation based large language model benchmarked on a novel dataset.pdf', 'total_pages': 9, 'page': 7, 'page_label': '8', 'source_file': 'A retrieval-augmented generation based large language model benchmarked on a novel dataset.pdf', 'file_type': 'pdf'}, page_content='ing Palm with OpenAI embed. While on paper it makes sense that Palm would work better with Palm embed, it actually \\nperformed better when paired with OpenAI‚Äôs embed. 0.91 (with OpenAI embed) versus 0.88 (Palm embed)‚Äînote that \\nthis is without context given. A similar eÔ¨Äect was noticed going the other way around as well, without context, GPT  \\nperformed much better with Palm embedding layer than with its own OpenAI embedding layer (0.897 versus 0.75 \\nrespectively). This data shows how Palm embedding layer tends to perform much better given no context when com-\\npared to OpenAI ‚Äôs embed. Similarly, to explored above, it is the opposite when given context, however. OpenAI‚Äôs \\nembedding layer performs a bit better when given context across the board than Palm embed. \\nMost notably from this experiment was two realizations. First, that Palm Embedding layer tends to work much \\nbetter when just given its own proprietary dataset (and no context), when compared to OpenAI‚Äôs embed. Additionally,'),\n",
       " Document(metadata={'producer': 'Adobe PDF Library 23.8.53', 'creator': 'Acrobat PDFMaker 23 for Word', 'creationdate': '2024-02-21T12:54:50-06:00', 'author': 'JOFSR', 'comments': '', 'company': '', 'keywords': '', 'moddate': '2024-02-21T12:55:38-06:00', 'sourcemodified': 'D:20240221185350', 'subject': '', 'title': '', 'source': '..\\\\data\\\\pdf\\\\A retrieval-augmented generation based large language model benchmarked on a novel dataset.pdf', 'total_pages': 9, 'page': 7, 'page_label': '8', 'source_file': 'A retrieval-augmented generation based large language model benchmarked on a novel dataset.pdf', 'file_type': 'pdf'}, page_content='better when just given its own proprietary dataset (and no context), when compared to OpenAI‚Äôs embed. Additionally, \\nwhen given context, the playing Ô¨Åeld switches: Palm tends to perform much worse when given context when compared \\nto OpenAI. Lastly, it is important to note that a combination of Palm/GPT with OpenAI‚Äôs embedding layer and context \\nyielded extremely accurate results when its similarity scores were averaged. \\n \\nConclusion \\n \\nBuilding upon the foundation laid by our initial Ô¨Åndings, it is paramount to recognize the exceptional performance of \\nthe Palm model when utilizing its proprietary dataset in conjunction with the Palm embed. This speciÔ¨Åcity in data and \\ntechnology synchronization has shown that Palm outshines OpenAI in terms of model accuracy in a context-free envi-\\nronment. However, the landscape shifts when contextual data is integrated. In such scenarios, the combination of GPT'),\n",
       " Document(metadata={'producer': 'Adobe PDF Library 23.8.53', 'creator': 'Acrobat PDFMaker 23 for Word', 'creationdate': '2024-02-21T12:54:50-06:00', 'author': 'JOFSR', 'comments': '', 'company': '', 'keywords': '', 'moddate': '2024-02-21T12:55:38-06:00', 'sourcemodified': 'D:20240221185350', 'subject': '', 'title': '', 'source': '..\\\\data\\\\pdf\\\\A retrieval-augmented generation based large language model benchmarked on a novel dataset.pdf', 'total_pages': 9, 'page': 7, 'page_label': '8', 'source_file': 'A retrieval-augmented generation based large language model benchmarked on a novel dataset.pdf', 'file_type': 'pdf'}, page_content='ronment. However, the landscape shifts when contextual data is integrated. In such scenarios, the combination of GPT \\nwith its native OpenAI embedding layer excels, leveraging the additional context to produce responses of remarkable \\naccuracy that resonate with the cultural and ecological nuances of the Amazon. \\nThis pivot in performance based on context underscores the signiÔ¨Åcance of tailored datasets and embedding \\nmechanisms in the optimization of Large Language Models (LLMs). The adaptability of the Retrieval -Augmented \\nGeneration (RAG) framework emerges as a cornerstone for future enhancements in LLMs. By enabling the seamless \\ninterchange of model components, RAG presents an evolutionary leap in the Ô¨Åne-tuning of language models, catering \\nto the intricate demands of culturally rich and contextually complex datasets. \\nIn light of these advancements, our research signiÔ¨Åes a pivotal moment for LLMs. The evidence suggests that'),\n",
       " Document(metadata={'producer': 'Adobe PDF Library 23.8.53', 'creator': 'Acrobat PDFMaker 23 for Word', 'creationdate': '2024-02-21T12:54:50-06:00', 'author': 'JOFSR', 'comments': '', 'company': '', 'keywords': '', 'moddate': '2024-02-21T12:55:38-06:00', 'sourcemodified': 'D:20240221185350', 'subject': '', 'title': '', 'source': '..\\\\data\\\\pdf\\\\A retrieval-augmented generation based large language model benchmarked on a novel dataset.pdf', 'total_pages': 9, 'page': 7, 'page_label': '8', 'source_file': 'A retrieval-augmented generation based large language model benchmarked on a novel dataset.pdf', 'file_type': 'pdf'}, page_content=\"to the intricate demands of culturally rich and contextually complex datasets. \\nIn light of these advancements, our research signiÔ¨Åes a pivotal moment for LLMs. The evidence suggests that \\nwhen models are Ô¨Ånely tuned with an awareness of the dataset's inherent context and the corresponding embedding \\nlayers, they reach new heights of linguistic precision. Therefore, the path forward for LLMs lies in embracing the \\nmodular and contextually aware RAG framework, which promises to reÔ¨Åne the capabilities of language models to an \\nunprecedented degree, ensuring the preservation and celebration of the world's diverse linguistic and cultural heritage. \\nVolume 12 Issue 4 (2023) \\nISSN: 2167-1907\\nwww.JSR.org/hs\\n8\"),\n",
       " Document(metadata={'producer': 'Adobe PDF Library 23.8.53', 'creator': 'Acrobat PDFMaker 23 for Word', 'creationdate': '2024-02-21T12:54:50-06:00', 'author': 'JOFSR', 'comments': '', 'company': '', 'keywords': '', 'moddate': '2024-02-21T12:55:38-06:00', 'sourcemodified': 'D:20240221185350', 'subject': '', 'title': '', 'source': '..\\\\data\\\\pdf\\\\A retrieval-augmented generation based large language model benchmarked on a novel dataset.pdf', 'total_pages': 9, 'page': 8, 'page_label': '9', 'source_file': 'A retrieval-augmented generation based large language model benchmarked on a novel dataset.pdf', 'file_type': 'pdf'}, page_content='Limitations \\n \\nThe results of the ‚Äúoutputted answer‚Äù of this algorithm largely reÔ¨Çect the quality of the data. If the LLM is trained oÔ¨Ä \\nlow-quality data, then the answer will reÔ¨Çect this bias. The results of the experiment will Ô¨Çuctuate with diÔ¨Äerent results \\nshould a diÔ¨Äerent dataset be used to train the LLM.  \\nIn the future there is a lot of potential to expand on this research by breaking down the RAG algorithm into \\nev\\nen more separate components to further see the diÔ¨Äerences in average similarity score that adding or removing each \\ncomponent makes. \\n \\nRefer ences \\n \\nBahdanau, D. C. (2016). End-to-end attention-based large vocabulary speech recognition. 2016 IEEE international \\nconference on acoustics, speech and signal processing (ICASSP), 4945-4949. \\nDevlin, J. C. (2018). Bert: Pre-training of deep bidirectional transformers for language understanding. arXiv preprint, \\narXiv:1810.04805.'),\n",
       " Document(metadata={'producer': 'Adobe PDF Library 23.8.53', 'creator': 'Acrobat PDFMaker 23 for Word', 'creationdate': '2024-02-21T12:54:50-06:00', 'author': 'JOFSR', 'comments': '', 'company': '', 'keywords': '', 'moddate': '2024-02-21T12:55:38-06:00', 'sourcemodified': 'D:20240221185350', 'subject': '', 'title': '', 'source': '..\\\\data\\\\pdf\\\\A retrieval-augmented generation based large language model benchmarked on a novel dataset.pdf', 'total_pages': 9, 'page': 8, 'page_label': '9', 'source_file': 'A retrieval-augmented generation based large language model benchmarked on a novel dataset.pdf', 'file_type': 'pdf'}, page_content='Devlin, J. C. (2018). Bert: Pre-training of deep bidirectional transformers for language understanding. arXiv preprint, \\narXiv:1810.04805. \\nLewis, P. P. (2020). Retrieval-augmented generation for knowledge-intensive nlp tasks. Advances in Neural \\nInformation Processing Systems, 9459-9474. \\nRadford, A. N. (2018). Improving language understanding by generative pre-training. OpenAI. \\nSiriwardhana, S. W. (2023). Improving the domain adaptation of retrieval augmented generation (RAG) models for \\nopen domain question answering. Transactions of the Association for Computational Linguistics, 1-17. \\nVaswani, A. S. (2017). Attention is all you need. Advances in neural information processing systems. \\nYu, W. (2022). Retrieval-augmented generation across heterogeneous knowledge. Proceedings of the 2022 \\nConference of the North American Chapter of the Association for Computational Linguistics: Human Language \\nTechnologies: Student Research Workshop, 52-58. \\n \\n \\n  \\nVolume 12 Issue 4 (2023)'),\n",
       " Document(metadata={'producer': 'Adobe PDF Library 23.8.53', 'creator': 'Acrobat PDFMaker 23 for Word', 'creationdate': '2024-02-21T12:54:50-06:00', 'author': 'JOFSR', 'comments': '', 'company': '', 'keywords': '', 'moddate': '2024-02-21T12:55:38-06:00', 'sourcemodified': 'D:20240221185350', 'subject': '', 'title': '', 'source': '..\\\\data\\\\pdf\\\\A retrieval-augmented generation based large language model benchmarked on a novel dataset.pdf', 'total_pages': 9, 'page': 8, 'page_label': '9', 'source_file': 'A retrieval-augmented generation based large language model benchmarked on a novel dataset.pdf', 'file_type': 'pdf'}, page_content='Conference of the North American Chapter of the Association for Computational Linguistics: Human Language \\nTechnologies: Student Research Workshop, 52-58. \\n \\n \\n  \\nVolume 12 Issue 4 (2023) \\nISSN: 2167-1907\\nwww.JSR.org/hs\\n9')]"
      ]
     },
     "execution_count": 14,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "chunks"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "id": "5bde24ed",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Generating embeddings for 41 texts...\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Batches: 100%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà| 2/2 [00:17<00:00,  8.61s/it]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Generated embeddings with shape: (41, 384)\n",
      "Adding 41 documents to vector store...\n",
      "Successfully added 41 documents to vector store\n",
      "Total documents in collection: 41\n"
     ]
    }
   ],
   "source": [
    "### Convert the text to embeddings\n",
    "texts = [doc.page_content for doc in chunks]\n",
    "\n",
    "## Generate the Embeddings\n",
    "embeddings = embedding_manager.generate_embeddings(texts)\n",
    "\n",
    "### store in the vector database\n",
    "vectorstore.add_documents(chunks, embeddings)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "498acd10",
   "metadata": {},
   "source": [
    "### Retriever Pipeline From VectorStore"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "id": "0f7b0ee2",
   "metadata": {},
   "outputs": [],
   "source": [
    "class RAGRetriever:\n",
    "    \"\"\"Handles query-based retrieval from the vector store\"\"\"\n",
    "    \n",
    "    def __init__(self, vector_store: VectorStore, embedding_manager: EmbeddingManager):\n",
    "        \"\"\"\n",
    "        Initialize the retriever\n",
    "        \n",
    "        Args:\n",
    "            vector_store: Vector store containing document embeddings\n",
    "            embedding_manager: Manager for generating query embeddings\n",
    "        \"\"\"\n",
    "        self.vector_store = vector_store\n",
    "        self.embedding_manager = embedding_manager\n",
    "\n",
    "    def retrieve(self, query: str, top_k: int = 5, score_threshold: float = 0.0) -> List[Dict[str, Any]]:\n",
    "        \"\"\"\n",
    "        Retrieve relevant documents for a query\n",
    "        \n",
    "        Args:\n",
    "            query: The search query\n",
    "            top_k: Number of top results to return\n",
    "            score_threshold: Minimum similarity score threshold\n",
    "            \n",
    "        Returns:\n",
    "            List of dictionaries containing retrieved documents and metadata\n",
    "        \"\"\"\n",
    "        print(f\"Retrieving documents for query: '{query}'\")\n",
    "        print(f\"Top K: {top_k}, Score threshold: {score_threshold}\")\n",
    "        \n",
    "        # Generate query embedding\n",
    "        query_embedding = self.embedding_manager.generate_embeddings([query])[0]\n",
    "        \n",
    "        # Search in vector store\n",
    "        try:\n",
    "            results = self.vector_store.collection.query(\n",
    "                query_embeddings=[query_embedding.tolist()],\n",
    "                n_results=top_k\n",
    "            )\n",
    "            \n",
    "            # Process results\n",
    "            retrieved_docs = []\n",
    "            \n",
    "            if results['documents'] and results['documents'][0]:\n",
    "                documents = results['documents'][0]\n",
    "                metadatas = results['metadatas'][0]\n",
    "                distances = results['distances'][0]\n",
    "                ids = results['ids'][0]\n",
    "                \n",
    "                for i, (doc_id, document, metadata, distance) in enumerate(zip(ids, documents, metadatas, distances)):\n",
    "                    # Convert distance to similarity score (ChromaDB uses cosine distance)\n",
    "                    similarity_score = 1 - distance\n",
    "                    \n",
    "                    if similarity_score >= score_threshold:\n",
    "                        retrieved_docs.append({\n",
    "                            'id': doc_id,\n",
    "                            'content': document,\n",
    "                            'metadata': metadata,\n",
    "                            'similarity_score': similarity_score,\n",
    "                            'distance': distance,\n",
    "                            'rank': i + 1\n",
    "                        })\n",
    "                \n",
    "                print(f\"Retrieved {len(retrieved_docs)} documents (after filtering)\")\n",
    "            else:\n",
    "                print(\"No documents found\")\n",
    "            \n",
    "            return retrieved_docs\n",
    "            \n",
    "        except Exception as e:\n",
    "            print(f\"Error during retrieval: {e}\")\n",
    "            return []\n",
    "\n",
    "rag_retriever=RAGRetriever(vectorstore,embedding_manager)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "id": "351730b4",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "<__main__.RAGRetriever at 0x2afe0c678c0>"
      ]
     },
     "execution_count": 17,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "rag_retriever"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "id": "f7e78529",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Retrieving documents for query: 'Advancements in RAG Framework'\n",
      "Top K: 1, Score threshold: 0.1\n",
      "Generating embeddings for 1 texts...\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Batches: 100%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà| 1/1 [00:00<00:00,  4.41it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Generated embeddings with shape: (1, 384)\n",
      "Retrieved 0 documents (after filtering)\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "[]"
      ]
     },
     "execution_count": 21,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "rag_retriever.retrieve(\"Advancements in RAG Framework\", top_k=1, score_threshold=0.1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "288a6c3a",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "RAG-using-Langchain",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.13.1"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
